{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bf0811851ed64c77ab56fd6b4cc5d6d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bf6dfead73484ffc8689e797b729ca8d",
              "IPY_MODEL_33eb76bd9ba84cffbee58f1541c50b47",
              "IPY_MODEL_7c7dd8e4ea8849a4bacb07a4ef3da3b8"
            ],
            "layout": "IPY_MODEL_e78900dd165d46f5a18e2b465e5bc783"
          }
        },
        "bf6dfead73484ffc8689e797b729ca8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0bf10bb0a314a8e9bffe682ee4a4629",
            "placeholder": "​",
            "style": "IPY_MODEL_c2709fe8526a438b9ce4cc802ea42537",
            "value": "modules.json: 100%"
          }
        },
        "33eb76bd9ba84cffbee58f1541c50b47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42636d7d333142e5b944a9e0947b6d47",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e08bfa0453f74fc9ba05da6fa51f5055",
            "value": 349
          }
        },
        "7c7dd8e4ea8849a4bacb07a4ef3da3b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5cf7f200d94e443bbc4615c228852766",
            "placeholder": "​",
            "style": "IPY_MODEL_ee1172bd04134e988269fd2833411076",
            "value": " 349/349 [00:00&lt;00:00, 18.0kB/s]"
          }
        },
        "e78900dd165d46f5a18e2b465e5bc783": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0bf10bb0a314a8e9bffe682ee4a4629": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2709fe8526a438b9ce4cc802ea42537": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42636d7d333142e5b944a9e0947b6d47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e08bfa0453f74fc9ba05da6fa51f5055": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5cf7f200d94e443bbc4615c228852766": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee1172bd04134e988269fd2833411076": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "caf780f885b44466a9f1124a135ed83d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_852a1c1cf1b64bb1b47b2c36fd6eb4e7",
              "IPY_MODEL_99a0220c491940ab8e1b478929c75bc7",
              "IPY_MODEL_b9a487395d374b89835cf996797b4df5"
            ],
            "layout": "IPY_MODEL_114d29ebcd994d1088891e7ee3810264"
          }
        },
        "852a1c1cf1b64bb1b47b2c36fd6eb4e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a6a659366c84eb1a47f4c4a23a6c76c",
            "placeholder": "​",
            "style": "IPY_MODEL_a8325b7be05445e08bd7509c5b2ef1f5",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "99a0220c491940ab8e1b478929c75bc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_427b3f74863f46a2ae888c5af6afcfb4",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_707f2cec1af1460c9f5b9e8a29aba330",
            "value": 116
          }
        },
        "b9a487395d374b89835cf996797b4df5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70a3f4e75fc340a3be2edf20dbcccb8d",
            "placeholder": "​",
            "style": "IPY_MODEL_b3d469f2db5a45449ab56affacde3248",
            "value": " 116/116 [00:00&lt;00:00, 5.43kB/s]"
          }
        },
        "114d29ebcd994d1088891e7ee3810264": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a6a659366c84eb1a47f4c4a23a6c76c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8325b7be05445e08bd7509c5b2ef1f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "427b3f74863f46a2ae888c5af6afcfb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "707f2cec1af1460c9f5b9e8a29aba330": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "70a3f4e75fc340a3be2edf20dbcccb8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3d469f2db5a45449ab56affacde3248": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4ddd187d87b42aea5dd9306de873657": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3670ac873a5542ee9fd6afe477703ca5",
              "IPY_MODEL_f3d321e1ff8945b7a7233953b9d782c8",
              "IPY_MODEL_a5768b93586549beb02e5397d76e0494"
            ],
            "layout": "IPY_MODEL_1508ef80d42d4546b7a00c940f255fba"
          }
        },
        "3670ac873a5542ee9fd6afe477703ca5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe2b490eb74c49ddb455e5124260ba54",
            "placeholder": "​",
            "style": "IPY_MODEL_9b81d75b313d40b281f79957e5caa2de",
            "value": "README.md: "
          }
        },
        "f3d321e1ff8945b7a7233953b9d782c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06d0baa32999432b82734ff7af3afc2a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_71f21812be7b4754a737ad9319edf76b",
            "value": 1
          }
        },
        "a5768b93586549beb02e5397d76e0494": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2020a669d55486da65f78f903ce1fbc",
            "placeholder": "​",
            "style": "IPY_MODEL_d24527c4bf164e9286ff93016b530596",
            "value": " 10.5k/? [00:00&lt;00:00, 461kB/s]"
          }
        },
        "1508ef80d42d4546b7a00c940f255fba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe2b490eb74c49ddb455e5124260ba54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b81d75b313d40b281f79957e5caa2de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06d0baa32999432b82734ff7af3afc2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "71f21812be7b4754a737ad9319edf76b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c2020a669d55486da65f78f903ce1fbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d24527c4bf164e9286ff93016b530596": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24a004fb3a20464db66c7d30d07f651a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c35181c613824e6c958d05717eb328f5",
              "IPY_MODEL_f5a9e09e2ac045158f3b1499464cb353",
              "IPY_MODEL_79ef8efe67d4412baf678a27ec090ca8"
            ],
            "layout": "IPY_MODEL_6725d5b4dea0481d88a07fd8c7d98dc3"
          }
        },
        "c35181c613824e6c958d05717eb328f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fe4f7a803934697b0497ea0d9553e41",
            "placeholder": "​",
            "style": "IPY_MODEL_ed40275dfe8b4460b710bf9ef3fcd27d",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "f5a9e09e2ac045158f3b1499464cb353": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0611d6fa0c9a4cedac4da110b38ea508",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cfac674033a646c48aeb541589704e69",
            "value": 53
          }
        },
        "79ef8efe67d4412baf678a27ec090ca8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_087ded6e68904dd4afbccf962e196478",
            "placeholder": "​",
            "style": "IPY_MODEL_3c21db35b50a4b1c88816aedb5fc5546",
            "value": " 53.0/53.0 [00:00&lt;00:00, 2.46kB/s]"
          }
        },
        "6725d5b4dea0481d88a07fd8c7d98dc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fe4f7a803934697b0497ea0d9553e41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed40275dfe8b4460b710bf9ef3fcd27d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0611d6fa0c9a4cedac4da110b38ea508": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfac674033a646c48aeb541589704e69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "087ded6e68904dd4afbccf962e196478": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c21db35b50a4b1c88816aedb5fc5546": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e837a6692aba4de48044dfa1f467575e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_99632cf5631041a6a44574a5e18d0eae",
              "IPY_MODEL_3223aaba4b554ab58d0b793352e957d3",
              "IPY_MODEL_56fbc1259a9b463c86f65db373e6426a"
            ],
            "layout": "IPY_MODEL_f1795a29e3e248619d672e55fcbbd6e0"
          }
        },
        "99632cf5631041a6a44574a5e18d0eae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6318be0a08984074afbff27441f113dc",
            "placeholder": "​",
            "style": "IPY_MODEL_c78e28dc33484e229f68630794d3d823",
            "value": "config.json: 100%"
          }
        },
        "3223aaba4b554ab58d0b793352e957d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_005a01d7ff23428db796020eee89f4fb",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_98e96952f5964dc4ab69836cdd2fb5c5",
            "value": 612
          }
        },
        "56fbc1259a9b463c86f65db373e6426a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e62f3f0d8a4b4945ae9fbd73c1d2aad7",
            "placeholder": "​",
            "style": "IPY_MODEL_72ce1fc3cc8c491392a4df175fee7cd2",
            "value": " 612/612 [00:00&lt;00:00, 38.4kB/s]"
          }
        },
        "f1795a29e3e248619d672e55fcbbd6e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6318be0a08984074afbff27441f113dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c78e28dc33484e229f68630794d3d823": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "005a01d7ff23428db796020eee89f4fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98e96952f5964dc4ab69836cdd2fb5c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e62f3f0d8a4b4945ae9fbd73c1d2aad7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72ce1fc3cc8c491392a4df175fee7cd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6ac8e7c2afe42dda34c6e334b119976": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_71d7b6c7d8534bdcac8971117c9864b6",
              "IPY_MODEL_297e2b2124ac46bca9f7eacc7d1bf9db",
              "IPY_MODEL_04726e16834f4bde931f2b52c73735dc"
            ],
            "layout": "IPY_MODEL_d77ecf52ac5c4dbbaffbf98ce471c9ce"
          }
        },
        "71d7b6c7d8534bdcac8971117c9864b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9776782ca4224db9994b892234629dae",
            "placeholder": "​",
            "style": "IPY_MODEL_22196acf17f843a5b3e6b92530ece637",
            "value": "model.safetensors: 100%"
          }
        },
        "297e2b2124ac46bca9f7eacc7d1bf9db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4e41884f7e54cfb8ea73d11afb2fd0f",
            "max": 90868376,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d78d018826374d6d8f1d06d5e3a15689",
            "value": 90868376
          }
        },
        "04726e16834f4bde931f2b52c73735dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fa44c594e9b4709ad342c7724db0253",
            "placeholder": "​",
            "style": "IPY_MODEL_6542d8c8fa1b496eb07d7bd75f6a0dd6",
            "value": " 90.9M/90.9M [00:01&lt;00:00, 104MB/s]"
          }
        },
        "d77ecf52ac5c4dbbaffbf98ce471c9ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9776782ca4224db9994b892234629dae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22196acf17f843a5b3e6b92530ece637": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4e41884f7e54cfb8ea73d11afb2fd0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d78d018826374d6d8f1d06d5e3a15689": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4fa44c594e9b4709ad342c7724db0253": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6542d8c8fa1b496eb07d7bd75f6a0dd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92ca4c12c8f0499aabb702d542089dee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d74a97d96a77454090b4cf05aeef52f4",
              "IPY_MODEL_8575c7724c1e46f39ad7287004c7105d",
              "IPY_MODEL_a9adc823c809461ebbc3ef9cb0e97a41"
            ],
            "layout": "IPY_MODEL_64e0334145c44f379165718ea0691d56"
          }
        },
        "d74a97d96a77454090b4cf05aeef52f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e0318a13e57467bafec94e2315a2c1f",
            "placeholder": "​",
            "style": "IPY_MODEL_06080b8382f84d7caf8309a2c6bec0e6",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "8575c7724c1e46f39ad7287004c7105d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f01833fd481244948126b693872d519b",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3d6bb8d6a412475eb0d7bc2a332f50c8",
            "value": 350
          }
        },
        "a9adc823c809461ebbc3ef9cb0e97a41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_acb49919872a42fb9d1bd59e5b484bed",
            "placeholder": "​",
            "style": "IPY_MODEL_4ea366148a4847c89e179b60ccacabe1",
            "value": " 350/350 [00:00&lt;00:00, 29.8kB/s]"
          }
        },
        "64e0334145c44f379165718ea0691d56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e0318a13e57467bafec94e2315a2c1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06080b8382f84d7caf8309a2c6bec0e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f01833fd481244948126b693872d519b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d6bb8d6a412475eb0d7bc2a332f50c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "acb49919872a42fb9d1bd59e5b484bed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ea366148a4847c89e179b60ccacabe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "854b1fd551904ce889b8f340138b3947": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_095f256386734cdd8af8f31ed7a15f58",
              "IPY_MODEL_220936bd16cc4d44b62c92fb39caa0cc",
              "IPY_MODEL_6d822582177248ccbc397d4dba514386"
            ],
            "layout": "IPY_MODEL_5a952314f8cb4ea8bc0bd56a78dceff9"
          }
        },
        "095f256386734cdd8af8f31ed7a15f58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f58c8a4209f46738cd3d42b37db0e33",
            "placeholder": "​",
            "style": "IPY_MODEL_14a59572124e4d3f9de8d661156923dc",
            "value": "vocab.txt: "
          }
        },
        "220936bd16cc4d44b62c92fb39caa0cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f5392c4ad62460ca18b840d03fa0b23",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_341c44afd3da4d83bfcf3e7f5b106a35",
            "value": 1
          }
        },
        "6d822582177248ccbc397d4dba514386": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fd15a0e147447c48121343e7b20be17",
            "placeholder": "​",
            "style": "IPY_MODEL_0ad13e4d8f8540be9e02686d87cc028d",
            "value": " 232k/? [00:00&lt;00:00, 6.36MB/s]"
          }
        },
        "5a952314f8cb4ea8bc0bd56a78dceff9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f58c8a4209f46738cd3d42b37db0e33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14a59572124e4d3f9de8d661156923dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f5392c4ad62460ca18b840d03fa0b23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "341c44afd3da4d83bfcf3e7f5b106a35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1fd15a0e147447c48121343e7b20be17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ad13e4d8f8540be9e02686d87cc028d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8682de12aa53465ebd0d6f46e4ec8a7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1377b47582134cb996e634a7fea86642",
              "IPY_MODEL_77f46c45e21040788f10ade70858a33f",
              "IPY_MODEL_addec2ee04a84d76b64bf94e56855011"
            ],
            "layout": "IPY_MODEL_25c492f185174239ba0052160cdc541b"
          }
        },
        "1377b47582134cb996e634a7fea86642": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdeabb819ef94e699fe2f26e1399a24c",
            "placeholder": "​",
            "style": "IPY_MODEL_fba32fee74ad4cb39e091323f207153c",
            "value": "tokenizer.json: "
          }
        },
        "77f46c45e21040788f10ade70858a33f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8756f5c0b18f4bca86b231c02dbda881",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba10f5c43f9e48c7abe529d6d24b6f54",
            "value": 1
          }
        },
        "addec2ee04a84d76b64bf94e56855011": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72b8c7f764164cf1bbe8fc92975b6c89",
            "placeholder": "​",
            "style": "IPY_MODEL_9ced8afd635c4d8292ffcd54b6ff3b6a",
            "value": " 466k/? [00:00&lt;00:00, 16.3MB/s]"
          }
        },
        "25c492f185174239ba0052160cdc541b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdeabb819ef94e699fe2f26e1399a24c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fba32fee74ad4cb39e091323f207153c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8756f5c0b18f4bca86b231c02dbda881": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "ba10f5c43f9e48c7abe529d6d24b6f54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "72b8c7f764164cf1bbe8fc92975b6c89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ced8afd635c4d8292ffcd54b6ff3b6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a33b2f08dd5468fa32a2cfa4621a89d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a67bf1ed5cb240308de666d363860dbb",
              "IPY_MODEL_1d8bf06d30a1491ea6ecb1bdba680615",
              "IPY_MODEL_f12606172ff94be4bac9f833eb7fba31"
            ],
            "layout": "IPY_MODEL_95e2621c210744debf0ea9d784e30375"
          }
        },
        "a67bf1ed5cb240308de666d363860dbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f24b632578ee4ac89dddd00783900ad9",
            "placeholder": "​",
            "style": "IPY_MODEL_878b1b58291548aeb21344c083c995c0",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "1d8bf06d30a1491ea6ecb1bdba680615": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af8ddb22e6fb4f549109ff2dfd822dd1",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6eb823148b824729bbc8f2a6eaee801d",
            "value": 112
          }
        },
        "f12606172ff94be4bac9f833eb7fba31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7301c48c28ab43719d98cb464693c5fe",
            "placeholder": "​",
            "style": "IPY_MODEL_79b4a07010c149d0a8f1530b2aa67574",
            "value": " 112/112 [00:00&lt;00:00, 5.62kB/s]"
          }
        },
        "95e2621c210744debf0ea9d784e30375": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f24b632578ee4ac89dddd00783900ad9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "878b1b58291548aeb21344c083c995c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af8ddb22e6fb4f549109ff2dfd822dd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6eb823148b824729bbc8f2a6eaee801d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7301c48c28ab43719d98cb464693c5fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79b4a07010c149d0a8f1530b2aa67574": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5379aabae81a4781a13c4f34dd587606": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3cb1e1e55ae545bd9a4180cb66e3429a",
              "IPY_MODEL_9543f1474a4643c396a9018ef2e1c462",
              "IPY_MODEL_bf2793eee5464ed9952de560c813f5d4"
            ],
            "layout": "IPY_MODEL_41e6681c515d419fb2fdf1aba8fe2510"
          }
        },
        "3cb1e1e55ae545bd9a4180cb66e3429a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82851ac9409f4fc299991669886d1c15",
            "placeholder": "​",
            "style": "IPY_MODEL_10269ccce09046409a8f3b4fe6868cd4",
            "value": "config.json: 100%"
          }
        },
        "9543f1474a4643c396a9018ef2e1c462": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f9fc2d4759c4193b6151a67a21cc890",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e7ee53ac92464e1bb812d76456f52539",
            "value": 190
          }
        },
        "bf2793eee5464ed9952de560c813f5d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dcc974b01a21497ab794b5e8b0460044",
            "placeholder": "​",
            "style": "IPY_MODEL_7d3c627eb3364f9eb2c11456f89fd240",
            "value": " 190/190 [00:00&lt;00:00, 11.2kB/s]"
          }
        },
        "41e6681c515d419fb2fdf1aba8fe2510": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82851ac9409f4fc299991669886d1c15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10269ccce09046409a8f3b4fe6868cd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f9fc2d4759c4193b6151a67a21cc890": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7ee53ac92464e1bb812d76456f52539": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dcc974b01a21497ab794b5e8b0460044": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d3c627eb3364f9eb2c11456f89fd240": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hMyYN509Tw-A"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langgraph langchain-community langchain-text-splitters langchain-groq langchain-huggingface langchain-chroma pymupdf arxiv sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['GROQ_API_KEY'] = userdata.get('GROQ_API_KEY')\n",
        "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "gPXssxtkT9IS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Literal, List\n",
        "class AgentState(TypedDict):\n",
        "    messages: List[str]\n",
        "    query: str\n",
        "    retriever_choice: str\n",
        "    retrieved_docs: List[str]\n",
        "    final_answer: str"
      ],
      "metadata": {
        "id": "kCTzx7YDT_7m"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "llm = ChatGroq(model_name=\"openai/gpt-oss-120b\", temperature=0)"
      ],
      "metadata": {
        "id": "R8hnRUIqUF8E"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "loader = WebBaseLoader([\n",
        "    \"https://python.langchain.com/docs/introduction/\",\n",
        "    \"https://python.langchain.com/docs/tutorials/rag/\",\n",
        "    \"https://python.langchain.com/docs/how_to/\",\n",
        "])\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StFAj5WZUS1-",
        "outputId": "1aa9476c-3ec9-470c-dfed-72f83d446f34"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "splits = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "Z9DGawvVUWUG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "bf0811851ed64c77ab56fd6b4cc5d6d3",
            "bf6dfead73484ffc8689e797b729ca8d",
            "33eb76bd9ba84cffbee58f1541c50b47",
            "7c7dd8e4ea8849a4bacb07a4ef3da3b8",
            "e78900dd165d46f5a18e2b465e5bc783",
            "f0bf10bb0a314a8e9bffe682ee4a4629",
            "c2709fe8526a438b9ce4cc802ea42537",
            "42636d7d333142e5b944a9e0947b6d47",
            "e08bfa0453f74fc9ba05da6fa51f5055",
            "5cf7f200d94e443bbc4615c228852766",
            "ee1172bd04134e988269fd2833411076",
            "caf780f885b44466a9f1124a135ed83d",
            "852a1c1cf1b64bb1b47b2c36fd6eb4e7",
            "99a0220c491940ab8e1b478929c75bc7",
            "b9a487395d374b89835cf996797b4df5",
            "114d29ebcd994d1088891e7ee3810264",
            "8a6a659366c84eb1a47f4c4a23a6c76c",
            "a8325b7be05445e08bd7509c5b2ef1f5",
            "427b3f74863f46a2ae888c5af6afcfb4",
            "707f2cec1af1460c9f5b9e8a29aba330",
            "70a3f4e75fc340a3be2edf20dbcccb8d",
            "b3d469f2db5a45449ab56affacde3248",
            "d4ddd187d87b42aea5dd9306de873657",
            "3670ac873a5542ee9fd6afe477703ca5",
            "f3d321e1ff8945b7a7233953b9d782c8",
            "a5768b93586549beb02e5397d76e0494",
            "1508ef80d42d4546b7a00c940f255fba",
            "fe2b490eb74c49ddb455e5124260ba54",
            "9b81d75b313d40b281f79957e5caa2de",
            "06d0baa32999432b82734ff7af3afc2a",
            "71f21812be7b4754a737ad9319edf76b",
            "c2020a669d55486da65f78f903ce1fbc",
            "d24527c4bf164e9286ff93016b530596",
            "24a004fb3a20464db66c7d30d07f651a",
            "c35181c613824e6c958d05717eb328f5",
            "f5a9e09e2ac045158f3b1499464cb353",
            "79ef8efe67d4412baf678a27ec090ca8",
            "6725d5b4dea0481d88a07fd8c7d98dc3",
            "5fe4f7a803934697b0497ea0d9553e41",
            "ed40275dfe8b4460b710bf9ef3fcd27d",
            "0611d6fa0c9a4cedac4da110b38ea508",
            "cfac674033a646c48aeb541589704e69",
            "087ded6e68904dd4afbccf962e196478",
            "3c21db35b50a4b1c88816aedb5fc5546",
            "e837a6692aba4de48044dfa1f467575e",
            "99632cf5631041a6a44574a5e18d0eae",
            "3223aaba4b554ab58d0b793352e957d3",
            "56fbc1259a9b463c86f65db373e6426a",
            "f1795a29e3e248619d672e55fcbbd6e0",
            "6318be0a08984074afbff27441f113dc",
            "c78e28dc33484e229f68630794d3d823",
            "005a01d7ff23428db796020eee89f4fb",
            "98e96952f5964dc4ab69836cdd2fb5c5",
            "e62f3f0d8a4b4945ae9fbd73c1d2aad7",
            "72ce1fc3cc8c491392a4df175fee7cd2",
            "b6ac8e7c2afe42dda34c6e334b119976",
            "71d7b6c7d8534bdcac8971117c9864b6",
            "297e2b2124ac46bca9f7eacc7d1bf9db",
            "04726e16834f4bde931f2b52c73735dc",
            "d77ecf52ac5c4dbbaffbf98ce471c9ce",
            "9776782ca4224db9994b892234629dae",
            "22196acf17f843a5b3e6b92530ece637",
            "c4e41884f7e54cfb8ea73d11afb2fd0f",
            "d78d018826374d6d8f1d06d5e3a15689",
            "4fa44c594e9b4709ad342c7724db0253",
            "6542d8c8fa1b496eb07d7bd75f6a0dd6",
            "92ca4c12c8f0499aabb702d542089dee",
            "d74a97d96a77454090b4cf05aeef52f4",
            "8575c7724c1e46f39ad7287004c7105d",
            "a9adc823c809461ebbc3ef9cb0e97a41",
            "64e0334145c44f379165718ea0691d56",
            "7e0318a13e57467bafec94e2315a2c1f",
            "06080b8382f84d7caf8309a2c6bec0e6",
            "f01833fd481244948126b693872d519b",
            "3d6bb8d6a412475eb0d7bc2a332f50c8",
            "acb49919872a42fb9d1bd59e5b484bed",
            "4ea366148a4847c89e179b60ccacabe1",
            "854b1fd551904ce889b8f340138b3947",
            "095f256386734cdd8af8f31ed7a15f58",
            "220936bd16cc4d44b62c92fb39caa0cc",
            "6d822582177248ccbc397d4dba514386",
            "5a952314f8cb4ea8bc0bd56a78dceff9",
            "0f58c8a4209f46738cd3d42b37db0e33",
            "14a59572124e4d3f9de8d661156923dc",
            "3f5392c4ad62460ca18b840d03fa0b23",
            "341c44afd3da4d83bfcf3e7f5b106a35",
            "1fd15a0e147447c48121343e7b20be17",
            "0ad13e4d8f8540be9e02686d87cc028d",
            "8682de12aa53465ebd0d6f46e4ec8a7d",
            "1377b47582134cb996e634a7fea86642",
            "77f46c45e21040788f10ade70858a33f",
            "addec2ee04a84d76b64bf94e56855011",
            "25c492f185174239ba0052160cdc541b",
            "bdeabb819ef94e699fe2f26e1399a24c",
            "fba32fee74ad4cb39e091323f207153c",
            "8756f5c0b18f4bca86b231c02dbda881",
            "ba10f5c43f9e48c7abe529d6d24b6f54",
            "72b8c7f764164cf1bbe8fc92975b6c89",
            "9ced8afd635c4d8292ffcd54b6ff3b6a",
            "3a33b2f08dd5468fa32a2cfa4621a89d",
            "a67bf1ed5cb240308de666d363860dbb",
            "1d8bf06d30a1491ea6ecb1bdba680615",
            "f12606172ff94be4bac9f833eb7fba31",
            "95e2621c210744debf0ea9d784e30375",
            "f24b632578ee4ac89dddd00783900ad9",
            "878b1b58291548aeb21344c083c995c0",
            "af8ddb22e6fb4f549109ff2dfd822dd1",
            "6eb823148b824729bbc8f2a6eaee801d",
            "7301c48c28ab43719d98cb464693c5fe",
            "79b4a07010c149d0a8f1530b2aa67574",
            "5379aabae81a4781a13c4f34dd587606",
            "3cb1e1e55ae545bd9a4180cb66e3429a",
            "9543f1474a4643c396a9018ef2e1c462",
            "bf2793eee5464ed9952de560c813f5d4",
            "41e6681c515d419fb2fdf1aba8fe2510",
            "82851ac9409f4fc299991669886d1c15",
            "10269ccce09046409a8f3b4fe6868cd4",
            "3f9fc2d4759c4193b6151a67a21cc890",
            "e7ee53ac92464e1bb812d76456f52539",
            "dcc974b01a21497ab794b5e8b0460044",
            "7d3c627eb3364f9eb2c11456f89fd240"
          ]
        },
        "id": "Ht9uUhi7Uc6V",
        "outputId": "34dd5f86-69cb-47fa-ab0a-f880158b89fb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf0811851ed64c77ab56fd6b4cc5d6d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "caf780f885b44466a9f1124a135ed83d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4ddd187d87b42aea5dd9306de873657"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24a004fb3a20464db66c7d30d07f651a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e837a6692aba4de48044dfa1f467575e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b6ac8e7c2afe42dda34c6e334b119976"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92ca4c12c8f0499aabb702d542089dee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "854b1fd551904ce889b8f340138b3947"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8682de12aa53465ebd0d6f46e4ec8a7d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a33b2f08dd5468fa32a2cfa4621a89d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5379aabae81a4781a13c4f34dd587606"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=\"./chroma_db\"\n",
        ")"
      ],
      "metadata": {
        "id": "GGFQIFwGUlZv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Standard Similarity Retriever\n",
        "similarity_retriever = vectorstore.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 4}\n",
        ")"
      ],
      "metadata": {
        "id": "y0TpV5O2U0F8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.storage import InMemoryStore\n",
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "\n",
        "# 2. Parent Document Retriever\n",
        "store = InMemoryStore()\n",
        "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
        "\n",
        "parent_doc_retriever = ParentDocumentRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        "    parent_splitter=parent_splitter,\n",
        ")"
      ],
      "metadata": {
        "id": "-aYZppEsU3_R"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "# 3. Multi-Query Retriever\n",
        "multiquery_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=similarity_retriever,\n",
        "    llm=llm\n",
        ")"
      ],
      "metadata": {
        "id": "SHaFB-2kVCwp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
        "\n",
        "# 4. Self-Query Retriever (simplified version)\n",
        "selfquery_retriever = vectorstore.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs={\"k\": 4, \"lambda_mult\": 0.25}\n",
        ")"
      ],
      "metadata": {
        "id": "WG6udus3VIdH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. HyDE-style retriever (hypothetical document embeddings simulation)\n",
        "hyde_retriever = vectorstore.as_retriever(\n",
        "    search_type=\"similarity_score_threshold\",\n",
        "    search_kwargs={\"score_threshold\": 0.5, \"k\": 3}\n",
        ")"
      ],
      "metadata": {
        "id": "ozV2y9A8VR16"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parent_doc_retrieve(state: AgentState) -> AgentState:\n",
        "    \"\"\"Retrieve using Parent Document Retriever for comprehensive context\"\"\"\n",
        "    query = state[\"query\"]\n",
        "    docs = parent_doc_retriever.get_relevant_documents(query)\n",
        "    retrieved_content = [doc.page_content for doc in docs]\n",
        "\n",
        "    state[\"retrieved_docs\"] = retrieved_content\n",
        "    state[\"retriever_choice\"] = \"parent_document\"\n",
        "    return state\n",
        "\n",
        "def multiquery_retrieve(state: AgentState) -> AgentState:\n",
        "    \"\"\"Retrieve using Multi-Query for query expansion\"\"\"\n",
        "    query = state[\"query\"]\n",
        "    docs = multiquery_retriever.get_relevant_documents(query)\n",
        "    retrieved_content = [doc.page_content for doc in docs]\n",
        "\n",
        "    state[\"retrieved_docs\"] = retrieved_content\n",
        "    state[\"retriever_choice\"] = \"multi_query\"\n",
        "    return state\n",
        "\n",
        "def selfquery_retrieve(state: AgentState) -> AgentState:\n",
        "    \"\"\"Retrieve using Self-Query with MMR for diversity\"\"\"\n",
        "    query = state[\"query\"]\n",
        "    docs = selfquery_retriever.get_relevant_documents(query)\n",
        "    retrieved_content = [doc.page_content for doc in docs]\n",
        "\n",
        "    state[\"retrieved_docs\"] = retrieved_content\n",
        "    state[\"retriever_choice\"] = \"self_query\"\n",
        "    return state\n",
        "\n",
        "def hyde_retrieve(state: AgentState) -> AgentState:\n",
        "    \"\"\"Retrieve using HyDE-style approach with score threshold\"\"\"\n",
        "    query = state[\"query\"]\n",
        "    docs = hyde_retriever.get_relevant_documents(query)\n",
        "    retrieved_content = [doc.page_content for doc in docs]\n",
        "\n",
        "    state[\"retrieved_docs\"] = retrieved_content\n",
        "    state[\"retriever_choice\"] = \"hyde\"\n",
        "    return state\n",
        "\n",
        "def similarity_retrieve(state: AgentState) -> AgentState:\n",
        "    \"\"\"Default similarity retriever\"\"\"\n",
        "    query = state[\"query\"]\n",
        "    docs = similarity_retriever.get_relevant_documents(query)\n",
        "    retrieved_content = [doc.page_content for doc in docs]\n",
        "\n",
        "    state[\"retrieved_docs\"] = retrieved_content\n",
        "    state[\"retriever_choice\"] = \"similarity\"\n",
        "    return state"
      ],
      "metadata": {
        "id": "TMI6MjCiVSgq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.types import Command\n",
        "\n",
        "def supervisor_route(state: AgentState) -> Command[Literal[\"parent_doc_retrieve\", \"multiquery_retrieve\", \"selfquery_retrieve\", \"hyde_retrieve\", \"similarity_retrieve\"]]:\n",
        "    \"\"\"\n",
        "    Supervisor agent that routes queries to appropriate retrievers based on query characteristics\n",
        "    \"\"\"\n",
        "    query = state[\"query\"].lower()\n",
        "\n",
        "    # Routing logic based on query analysis\n",
        "    if any(word in query for word in [\"comprehensive\", \"detailed\", \"complete\", \"full context\"]):\n",
        "        return Command(goto=\"parent_doc_retrieve\")\n",
        "\n",
        "    elif any(word in query for word in [\"variations\", \"different ways\", \"alternatives\", \"multiple\"]):\n",
        "        return Command(goto=\"multiquery_retrieve\")\n",
        "\n",
        "    elif any(word in query for word in [\"diverse\", \"variety\", \"different perspectives\", \"broad\"]):\n",
        "        return Command(goto=\"selfquery_retrieve\")\n",
        "\n",
        "    elif any(word in query for word in [\"hypothetical\", \"what if\", \"suppose\", \"imagine\"]):\n",
        "        return Command(goto=\"hyde_retrieve\")\n",
        "\n",
        "    else:\n",
        "        return Command(goto=\"similarity_retrieve\")"
      ],
      "metadata": {
        "id": "4kMTpYXeVVlQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(state: AgentState) -> AgentState:\n",
        "    \"\"\"Generate final answer using retrieved documents\"\"\"\n",
        "    query = state[\"query\"]\n",
        "    docs = state[\"retrieved_docs\"]\n",
        "    retriever_used = state[\"retriever_choice\"]\n",
        "\n",
        "    context = \"\\n\\n\".join(docs[:3])  # Use top 3 documents\n",
        "\n",
        "    prompt = f\"\"\"Based on the following context, answer the question comprehensively.\n",
        "\n",
        "Context from {retriever_used} retriever:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "    state[\"final_answer\"] = response.content\n",
        "    return state"
      ],
      "metadata": {
        "id": "O0QNamVPVeQ5"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"supervisor\", supervisor_route)\n",
        "workflow.add_node(\"parent_doc_retrieve\", parent_doc_retrieve)\n",
        "workflow.add_node(\"multiquery_retrieve\", multiquery_retrieve)\n",
        "workflow.add_node(\"selfquery_retrieve\", selfquery_retrieve)\n",
        "workflow.add_node(\"hyde_retrieve\", hyde_retrieve)\n",
        "workflow.add_node(\"similarity_retrieve\", similarity_retrieve)\n",
        "workflow.add_node(\"generate_answer\", generate_answer)\n",
        "\n",
        "workflow.add_edge(START, \"supervisor\")\n",
        "\n",
        "workflow.add_edge(\"parent_doc_retrieve\", \"generate_answer\")\n",
        "workflow.add_edge(\"multiquery_retrieve\", \"generate_answer\")\n",
        "workflow.add_edge(\"selfquery_retrieve\", \"generate_answer\")\n",
        "workflow.add_edge(\"hyde_retrieve\", \"generate_answer\")\n",
        "workflow.add_edge(\"similarity_retrieve\", \"generate_answer\")\n",
        "\n",
        "workflow.add_edge(\"generate_answer\", END)\n",
        "\n",
        "app = workflow.compile()"
      ],
      "metadata": {
        "id": "p839gkkQViD8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_rag_query(query: str):\n",
        "    \"\"\"Run a query through the multi-retriever RAG system\"\"\"\n",
        "    initial_state = AgentState(\n",
        "        messages=[],\n",
        "        query=query,\n",
        "        retriever_choice=\"\",\n",
        "        retrieved_docs=[],\n",
        "        final_answer=\"\"\n",
        "    )\n",
        "\n",
        "    result = app.invoke(initial_state)\n",
        "\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Retriever Used: {result['retriever_choice']}\")\n",
        "    print(f\"Answer: {result['final_answer']}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    return result\n",
        "\n",
        "test_queries = [\n",
        "    \"What is LangChain and how does it work?\",  # Should use similarity\n",
        "    \"Give me comprehensive details about RAG implementation\",  # Should use parent_doc\n",
        "    \"What are different ways to implement retrieval?\",  # Should use multiquery\n",
        "    \"Show me diverse approaches to document processing\",  # Should use selfquery\n",
        "    \"What if I wanted to create hypothetical documents?\",  # Should use hyde\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    run_rag_query(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmVcSLNRVtXA",
        "outputId": "9c2d3763-e7e1-4107-c3f5-395bf42f57ae"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2279847023.py:44: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  docs = similarity_retriever.get_relevant_documents(query)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What is LangChain and how does it work?\n",
            "Retriever Used: similarity\n",
            "Answer: **LangChain – a high‑level framework for building LLM‑powered applications**\n",
            "\n",
            "---\n",
            "\n",
            "## 1. What is LangChain?\n",
            "\n",
            "LangChain is an open‑source Python (and now also JavaScript/TypeScript) framework that makes it easier to **design, develop, and deploy applications that use large language models (LLMs)** such as OpenAI’s GPT‑4, Anthropic’s Claude, Llama‑2, Gemini, etc.  \n",
            "\n",
            "Instead of treating an LLM as a single “chat‑completion” endpoint, LangChain treats it as **one component in a larger data‑flow pipeline** and provides reusable building blocks for:\n",
            "\n",
            "| Stage | What it does | Typical LangChain component |\n",
            "|-------|--------------|-----------------------------|\n",
            "| **Prompt creation** | Assemble dynamic, context‑aware prompts | PromptTemplate, FewShotPromptTemplate |\n",
            "| **LLM invocation** | Call the model (chat, completion, embeddings) | LLM wrappers (OpenAI, AzureOpenAI, HuggingFace, etc.) |\n",
            "| **Memory / State** | Keep track of prior interactions or external facts | ConversationBufferMemory, VectorStoreRetrieverMemory |\n",
            "| **Tool use / Agents** | Let the model call external APIs, run code, query databases | Tools, Agents (ZeroShotAgent, ReAct, StructuredChatAgent) |\n",
            "| **Retrieval** | Pull relevant documents or embeddings before prompting | VectorStore (FAISS, Pinecone, Chroma, etc.), Retriever |\n",
            "| **Output parsing** | Convert raw LLM text into structured data | OutputParser, PydanticOutputParser |\n",
            "| **Orchestration** | Chain the above pieces together into a reproducible workflow | Chain, SequentialChain, RouterChain, LangGraph (state‑machine graphs) |\n",
            "\n",
            "In short, **LangChain abstracts the “glue” that connects LLMs to the rest of your software stack**, letting you focus on the business logic rather than on low‑level API calls, prompt engineering, or data handling.\n",
            "\n",
            "---\n",
            "\n",
            "## 2. How does LangChain work? – The Core Concepts\n",
            "\n",
            "### 2.1 Prompt Templates\n",
            "- **Static templates** (`PromptTemplate`) let you define a prompt with placeholders (`{question}`, `{context}`) that are filled at runtime.\n",
            "- **Few‑shot templates** (`FewShotPromptTemplate`) automatically prepend a set of example Q/A pairs to improve model performance.\n",
            "- **Chat‑style templates** (`ChatPromptTemplate`) build messages for chat‑based LLMs (system, user, assistant roles).\n",
            "\n",
            "### 2.2 LLM Wrappers\n",
            "LangChain ships adapters for most major providers (OpenAI, Azure, Anthropic, Cohere, HuggingFace, etc.).  \n",
            "A wrapper handles:\n",
            "- Authentication & endpoint configuration\n",
            "- Parameter defaults (temperature, max tokens, stop sequences)\n",
            "- Unified `.invoke()` / `.stream()` interface\n",
            "\n",
            "### 2.3 Chains\n",
            "A **Chain** is a directed sequence of components where the output of one step becomes the input of the next.  \n",
            "Common built‑in chains:\n",
            "- **LLMChain** – prompt → LLM → raw text\n",
            "- **RetrievalQAChain** – retrieve docs → augment prompt → LLM → answer\n",
            "- **ConversationalRetrievalChain** – adds memory to RetrievalQA for multi‑turn chat\n",
            "\n",
            "Chains can be nested, enabling arbitrarily complex pipelines.\n",
            "\n",
            "### 2.4 Memory\n",
            "Memory objects store information across turns so that a conversational agent can refer back to earlier messages without re‑sending the whole history.  \n",
            "Examples:\n",
            "- `ConversationBufferMemory` (simple list of past messages)\n",
            "- `ConversationSummaryMemory` (summarizes long histories)\n",
            "- `VectorStoreRetrieverMemory` (stores embeddings of past interactions for semantic lookup)\n",
            "\n",
            "### 2.5 Retrieval & Vector Stores\n",
            "When you need **grounded** answers (e.g., from a knowledge base), LangChain can:\n",
            "1. **Embed** documents with an embedding model.\n",
            "2. Store embeddings in a **VectorStore** (FAISS, Pinecone, Weaviate, Chroma, etc.).\n",
            "3. Use a **Retriever** to fetch the top‑k most relevant chunks given a query.\n",
            "4. Insert those chunks into the prompt (often via a `PromptTemplate`).\n",
            "\n",
            "### 2.6 Agents & Tools\n",
            "Agents let the LLM **decide** which tool to call next, enabling:\n",
            "- **Search** (web, internal DB)\n",
            "- **Code execution** (Python REPL, Jupyter)\n",
            "- **API calls** (REST, GraphQL)\n",
            "- **File I/O**, etc.\n",
            "\n",
            "The classic pattern is the **ReAct** loop:\n",
            "1. LLM produces a “thought” + an action request.\n",
            "2. The framework executes the requested tool.\n",
            "3. The result is fed back to the LLM.\n",
            "4. Repeat until the LLM signals it has a final answer.\n",
            "\n",
            "LangChain provides a catalog of ready‑made tools and a generic `Tool` interface for you to plug in custom ones.\n",
            "\n",
            "### 2.7 Output Parsing\n",
            "LLM responses are plain text, but many applications need structured data (JSON, Pydantic models, SQL queries).  \n",
            "LangChain offers:\n",
            "- `RegexParser`, `JSONOutputParser`\n",
            "- `PydanticOutputParser` (auto‑validates against a Pydantic schema)\n",
            "- `StructuredOutputParser` (newer, model‑agnostic)\n",
            "\n",
            "### 2.8 LangGraph (state‑machine orchestration)\n",
            "For workflows that are **non‑linear** (branching, loops, conditional steps), LangChain introduced **LangGraph**, a lightweight graph‑engine that:\n",
            "- Represents each node as a LangChain component (Chain, Agent, Tool, etc.).\n",
            "- Stores a mutable **state** dict that flows between nodes.\n",
            "- Allows you to define **conditional edges** (`if state[\"needs_search\"]: go_to(search_node)`).\n",
            "- Supports **asynchronous execution** and **checkpointing** for long‑running pipelines.\n",
            "\n",
            "LangGraph essentially turns a LangChain pipeline into a **state‑machine**, making it easier to reason about complex, multi‑step LLM applications (e.g., multi‑turn planning, autonomous agents).\n",
            "\n",
            "---\n",
            "\n",
            "## 3. Typical End‑to‑End Flow\n",
            "\n",
            "Below is a canonical “question‑answer‑with‑retrieval” flow, illustrating how the pieces fit together:\n",
            "\n",
            "```\n",
            "User query\n",
            "   │\n",
            "   ▼\n",
            "Retriever (VectorStore)  <-- fetch relevant docs\n",
            "   │\n",
            "   ▼\n",
            "PromptTemplate (inject docs + query)\n",
            "   │\n",
            "   ▼\n",
            "LLM (e.g., gpt‑4o)\n",
            "   │\n",
            "   ▼\n",
            "OutputParser (extract answer)\n",
            "   │\n",
            "   ▼\n",
            "(Optional) Memory (store Q/A for future turns)\n",
            "   │\n",
            "   ▼\n",
            "Return answer to user\n",
            "```\n",
            "\n",
            "If the LLM decides it needs an external tool (e.g., a web search), an **Agent** intercepts the loop, runs the tool, and feeds the result back into the same chain.\n",
            "\n",
            "---\n",
            "\n",
            "## 4. Why Use LangChain?\n",
            "\n",
            "| Benefit | Explanation |\n",
            "|---------|-------------|\n",
            "| **Modularity** | Swap out any component (different LLM, vector store, memory) without rewriting the whole app. |\n",
            "| **Rapid prototyping** | Pre‑built chains and tutorials let you get a working prototype in minutes. |\n",
            "| **Scalability** | Works locally (FAISS) or in the cloud (Pinecone, DynamoDB, Azure Cognitive Search). |\n",
            "| **Extensibility** | Add custom tools, parsers, or even new chain types; LangGraph lets you model arbitrary workflows. |\n",
            "| **Community & Ecosystem** | Rich set of integrations (LangChain Hub for sharing prompts/chains, LangGraph tutorials, extensive docs). |\n",
            "| **Production‑ready** | Supports async execution, streaming responses, checkpointing, and logging for observability. |\n",
            "\n",
            "---\n",
            "\n",
            "## 5. Quick Code Sketch (Python)\n",
            "\n",
            "```python\n",
            "from langchain import PromptTemplate, LLMChain\n",
            "from langchain.llms import OpenAI\n",
            "from langchain.vectorstores import FAISS\n",
            "from langchain.embeddings import OpenAIEmbeddings\n",
            "from langchain.retrievers import VectorStoreRetriever\n",
            "from langchain.memory import ConversationBufferMemory\n",
            "from langchain.chains import RetrievalQAChain\n",
            "\n",
            "# 1️⃣  Embed and store docs (run once)\n",
            "docs = [\"LangChain is a framework...\", \"LangGraph adds graph orchestration...\"]\n",
            "embeddings = OpenAIEmbeddings()\n",
            "vectorstore = FAISS.from_texts(docs, embeddings)\n",
            "\n",
            "# 2️⃣  Build retriever\n",
            "retriever = VectorStoreRetriever(vectorstore=vectorstore, k=2)\n",
            "\n",
            "# 3️⃣  Prompt template that expects a retrieved context\n",
            "prompt = PromptTemplate(\n",
            "    template=\"\"\"\n",
            "    Use the following context to answer the question.\n",
            "    Context: {context}\n",
            "    Question: {question}\n",
            "    Answer:\"\"\",\n",
            "    input_variables=[\"context\", \"question\"]\n",
            ")\n",
            "\n",
            "# 4️⃣  LLM wrapper\n",
            "llm = OpenAI(model=\"gpt-4o\", temperature=0)\n",
            "\n",
            "# 5️⃣  Chain that ties everything together\n",
            "qa_chain = RetrievalQAChain.from_chain_type(\n",
            "    llm=llm,\n",
            "    retriever=retriever,\n",
            "    chain_type=\"stuff\",          # simple concat of retrieved docs\n",
            "    return_source_documents=True,\n",
            "    combine_prompt=prompt\n",
            ")\n",
            "\n",
            "# 6️⃣  Optional memory for multi‑turn chat\n",
            "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
            "\n",
            "# 7️⃣  Ask a question\n",
            "result = qa_chain({\"question\": \"What is LangGraph?\"})\n",
            "print(result[\"answer\"])\n",
            "```\n",
            "\n",
            "The same logic can be expressed with LangGraph if you need branching (e.g., “if the answer confidence < 0.7 → run a web search”).\n",
            "\n",
            "---\n",
            "\n",
            "## 6. Summary\n",
            "\n",
            "- **LangChain** is a **framework** that abstracts the entire LLM application lifecycle: prompt engineering, model invocation, retrieval, memory, tool use, parsing, and orchestration.\n",
            "- It works by **composing reusable components** (PromptTemplates, LLM wrappers, Chains, Memory, Retrievers, Agents, OutputParsers) into pipelines.\n",
            "- **LangGraph** extends this composition into a **state‑machine graph**, enabling complex, conditional, and looped workflows.\n",
            "- The result is a **modular, extensible, and production‑ready** way to build anything from a simple Q&A bot to autonomous agents that browse the web, run code, and make decisions.\n",
            "\n",
            "In essence, LangChain lets developers treat an LLM **like any other software service**—plug it into a data pipeline, give it memory, let it call tools, and orchestrate the whole process with clear, testable building blocks.\n",
            "--------------------------------------------------------------------------------\n",
            "Query: Give me comprehensive details about RAG implementation\n",
            "Retriever Used: parent_document\n",
            "Answer: Below is a **comprehensive guide to building a Retrieval‑Augmented Generation (RAG) system** – from the high‑level concepts down to concrete implementation steps, code snippets, tooling choices, scaling considerations, evaluation, and best‑practice recommendations.  \n",
            "\n",
            "---\n",
            "\n",
            "## 1. What is RAG?\n",
            "\n",
            "| Component | Role |\n",
            "|-----------|------|\n",
            "| **Retriever** | Finds the most relevant pieces of external knowledge (documents, passages, tables, code snippets, etc.) given a user query. |\n",
            "| **Generator** | A large language model (LLM) that consumes the retrieved text (often as part of the prompt) and produces a grounded, fluent answer. |\n",
            "| **RAG Loop** | The retriever‑generator pipeline is executed for each user request, allowing the LLM to “look up” facts it does not store internally, dramatically improving factuality and domain‑specificity. |\n",
            "\n",
            "**Why use RAG?**  \n",
            "- Extends the knowledge horizon of a closed‑book LLM (no need to fine‑tune the model on every new document).  \n",
            "- Keeps the model up‑to‑date: you only re‑index new data, not re‑train.  \n",
            "- Enables domain‑specific QA, chatbots, code assistants, legal research, etc., while still leveraging the generative power of LLMs.\n",
            "\n",
            "---\n",
            "\n",
            "## 2. High‑Level Architecture\n",
            "\n",
            "```\n",
            "+-------------------+          +-------------------+          +-------------------+\n",
            "|   User Query      |  --->    |   Retriever       |  --->    |   Retrieved Docs  |\n",
            "+-------------------+          +-------------------+          +-------------------+\n",
            "                                   |                               |\n",
            "                                   | (vector / BM25 / hybrid)      |\n",
            "                                   v                               v\n",
            "+-------------------+          +-------------------+          +-------------------+\n",
            "|   Prompt Builder  |  <---    |   Retrieved Docs  |  <---    |   Knowledge Base |\n",
            "+-------------------+          +-------------------+          +-------------------+\n",
            "                                   |\n",
            "                                   v\n",
            "+-------------------+          +-------------------+\n",
            "|   LLM Generator   |  <---    |   Prompt (query +|\n",
            "+-------------------+          |   docs)          |\n",
            "                                   |\n",
            "                                   v\n",
            "+-------------------+\n",
            "|   Final Answer   |\n",
            "+-------------------+\n",
            "```\n",
            "\n",
            "- **Knowledge Base**: Raw documents (PDF, HTML, CSV, code, etc.).  \n",
            "- **Indexer**: Turns each document into *chunks* → embeddings → stored in a vector store (FAISS, Milvus, Pinecone, etc.) and optionally a lexical index (BM25).  \n",
            "- **Retriever**: Takes the user query, creates a query embedding, performs similarity search (top‑k) and/or lexical search, returns the most relevant chunks.  \n",
            "- **Prompt Builder**: Formats the retrieved chunks (often with citations) and the original query into a prompt that respects the LLM’s context window.  \n",
            "- **Generator**: Any LLM (OpenAI GPT‑4, Anthropic Claude, Llama‑2, Mistral, etc.) that can be called via API or locally.  \n",
            "\n",
            "---\n",
            "\n",
            "## 3. Core Implementation Steps\n",
            "\n",
            "| Step | Description | Typical Choices |\n",
            "|------|-------------|-----------------|\n",
            "| **3.1. Data Ingestion** | Load raw files, clean HTML, extract text from PDFs, split code, etc. | `unstructured`, `pdfminer`, `BeautifulSoup`, `tika` |\n",
            "| **3.2. Chunking** | Split documents into manageable pieces (e.g., 200‑400 tokens) while preserving context. | Recursive character splitter, semantic splitter (e.g., `langchain.text_splitter`) |\n",
            "| **3.3. Embedding Generation** | Convert each chunk into a dense vector. | OpenAI `text-embedding-3-large`, `sentence‑transformers` (e.g., `all‑mpnet‑base‑v2`), `Instructor‑XL`, `Mistral‑Embedding` |\n",
            "| **3.4. Index Construction** | Store vectors in a fast similarity‑search engine; optionally add a BM25 index for lexical fallback. | FAISS (in‑memory), Milvus, Pinecone, Weaviate, ElasticSearch (hybrid) |\n",
            "| **3.5. Retrieval** | At query time, embed the query, perform top‑k similarity search, optionally re‑rank. | `faiss.search`, `milvus.search`, `pinecone.query`, hybrid scoring (`reciprocal rank fusion`) |\n",
            "| **3.6. Prompt Engineering** | Build a prompt that includes: <br>• System instructions (e.g., “Answer based only on the provided sources.”) <br>• Retrieved passages with citations <br>• The user question <br>• Optional “few‑shot” examples | Use Jinja2 templates, LangChain `PromptTemplate`, or custom string formatting |\n",
            "| **3.7. Generation** | Call the LLM with the constructed prompt. | OpenAI `ChatCompletion`, Anthropic `messages`, local `vLLM`/`llama.cpp` |\n",
            "| **3.8. Post‑Processing** | Strip citations, enforce answer length, add source list, detect hallucinations. | Regex, LLM‑based verification, `Guardrails` |\n",
            "| **3.9. Evaluation & Monitoring** | Track latency, relevance (R‑Precision, MAP), factuality (BLEU/ROUGE vs. gold, LLM‑based fact‑check). | `langchain-eval`, `Haystack` evaluation suite, custom dashboards (Grafana, Prometheus) |\n",
            "\n",
            "---\n",
            "\n",
            "## 4. Detailed Walk‑through with Code (Python)\n",
            "\n",
            "Below is a **minimal, end‑to‑end example** using **LangChain** + **FAISS** + **OpenAI embeddings** + **OpenAI GPT‑4**. The same logic can be swapped for Haystack, LlamaIndex, or any other stack.\n",
            "\n",
            "```python\n",
            "# --------------------------------------------------------------\n",
            "# 0️⃣ Install required packages (run once)\n",
            "# --------------------------------------------------------------\n",
            "# pip install langchain openai faiss-cpu tiktoken tqdm python-dotenv\n",
            "\n",
            "# --------------------------------------------------------------\n",
            "# 1️⃣ Load & Chunk Documents\n",
            "# --------------------------------------------------------------\n",
            "import os, glob\n",
            "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
            "from langchain.document_loaders import UnstructuredPDFLoader, TextLoader\n",
            "\n",
            "def load_documents(path_pattern: str):\n",
            "    docs = []\n",
            "    for file_path in glob.glob(path_pattern):\n",
            "        if file_path.lower().endswith('.pdf'):\n",
            "            loader = UnstructuredPDFLoader(file_path)\n",
            "        else:\n",
            "            loader = TextLoader(file_path)\n",
            "        docs.extend(loader.load())\n",
            "    return docs\n",
            "\n",
            "raw_docs = load_documents(\"data/**/*.pdf\")   # adjust glob as needed\n",
            "\n",
            "splitter = RecursiveCharacterTextSplitter(\n",
            "    chunk_size=400,      # ~ 600‑800 tokens\n",
            "    chunk_overlap=50,\n",
            "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
            ")\n",
            "chunks = splitter.split_documents(raw_docs)\n",
            "\n",
            "# --------------------------------------------------------------\n",
            "# 2️⃣ Create Embeddings & Build Vector Store (FAISS)\n",
            "# --------------------------------------------------------------\n",
            "from langchain.embeddings import OpenAIEmbeddings\n",
            "from langchain.vectorstores import FAISS\n",
            "\n",
            "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")  # cheap, high‑dim\n",
            "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
            "\n",
            "# Persist for later use\n",
            "vectorstore.save_local(\"faiss_index\")\n",
            "\n",
            "# --------------------------------------------------------------\n",
            "# 3️⃣ Retrieval – Hybrid (vector + BM25) (optional)\n",
            "# --------------------------------------------------------------\n",
            "# For pure vector search:\n",
            "def retrieve(query: str, k: int = 5):\n",
            "    return vectorstore.similarity_search(query, k=k)\n",
            "\n",
            "# --------------------------------------------------------------\n",
            "# 4️⃣ Prompt Template\n",
            "# --------------------------------------------------------------\n",
            "from langchain.prompts import PromptTemplate\n",
            "\n",
            "PROMPT = \"\"\"\n",
            "You are an expert assistant. Answer the question using ONLY the information\n",
            "provided in the retrieved passages. Cite each fact with the passage number\n",
            "(e.g., [1], [2]) that appears in the \"Sources\" section.\n",
            "\n",
            "Question:\n",
            "{question}\n",
            "\n",
            "Sources:\n",
            "{sources}\n",
            "\"\"\"\n",
            "\n",
            "prompt = PromptTemplate(\n",
            "    input_variables=[\"question\", \"sources\"],\n",
            "    template=PROMPT\n",
            ")\n",
            "\n",
            "# --------------------------------------------------------------\n",
            "# 5️⃣ LLM Wrapper\n",
            "# --------------------------------------------------------------\n",
            "from langchain.llms import OpenAI\n",
            "llm = OpenAI(model_name=\"gpt-4o-mini\", temperature=0.0, max_tokens=500)\n",
            "\n",
            "# --------------------------------------------------------------\n",
            "# 6️⃣ End‑to‑End RAG Function\n",
            "# --------------------------------------------------------------\n",
            "def rag_answer(question: str, top_k: int = 5):\n",
            "    # 1️⃣ Retrieve\n",
            "    docs = retrieve(question, k=top_k)\n",
            "\n",
            "    # 2️⃣ Build source string with citations\n",
            "    sources = \"\\n\".join([f\"[{i+1}] {doc.page_content.strip()}\" for i, doc in enumerate(docs)])\n",
            "\n",
            "    # 3️⃣ Render prompt\n",
            "    formatted_prompt = prompt.format(question=question, sources=sources)\n",
            "\n",
            "    # 4️⃣ Generate answer\n",
            "    answer = llm(formatted_prompt)\n",
            "\n",
            "    # 5️⃣ (Optional) Post‑process – extract citations\n",
            "    return answer.strip()\n",
            "\n",
            "# --------------------------------------------------------------\n",
            "# 7️⃣ Demo\n",
            "# --------------------------------------------------------------\n",
            "if __name__ == \"__main__\":\n",
            "    q = \"What are the main challenges of scaling transformer models to billions of parameters?\"\n",
            "    print(rag_answer(q, top_k=4))\n",
            "```\n",
            "\n",
            "### What the script does\n",
            "\n",
            "1. **Loads** PDFs (or any text) from a folder.  \n",
            "2. **Splits** them into overlapping chunks (400 characters ≈ 600‑800 tokens).  \n",
            "3. **Embeds** each chunk with OpenAI’s `text‑embedding‑3‑large`.  \n",
            "4. **Indexes** the embeddings in a FAISS flat index (persisted to disk).  \n",
            "5. **Retrieves** the top‑k most similar chunks for a user query.  \n",
            "6. **Builds** a prompt that forces the LLM to answer *only* from those sources and to cite them.  \n",
            "7. **Calls** GPT‑4o‑mini (or any other model) and returns the answer.\n",
            "\n",
            "You can replace any component:\n",
            "\n",
            "| Component | Alternatives |\n",
            "|-----------|--------------|\n",
            "| **Loaders** | `PyMuPDF`, `pdfplumber`, `docx2txt`, `pandas` (CSV), `GitHub` API for code |\n",
            "| **Chunker** | Semantic chunker (`langchain.text_splitter.SemanticChunker`), `nltk` sentence tokenizer |\n",
            "| **Embedding** | `sentence‑transformers` (`all‑mpnet‑base‑v2`), `Instructor‑XL`, `Mistral‑Embedding`, `Cohere` |\n",
            "| **Vector Store** | Milvus, Pinecone, Weaviate, Qdrant, Elastic (dense + lexical) |\n",
            "| **Retriever** | Hybrid (FAISS + BM25), Reranker (`cross‑encoder/ms‑marco-MiniLM-L-6-v2`) |\n",
            "| **LLM** | OpenAI, Anthropic, Cohere, Llama‑2‑70B (via vLLM), Mistral‑7B‑Instruct, Gemini |\n",
            "| **Prompt** | Few‑shot examples, chain‑of‑thought, “Self‑Ask” style, tool‑use instructions |\n",
            "| **Post‑Processing** | `Guardrails`, `LLM‑based fact‑checkers`, `OpenAI Moderation` |\n",
            "\n",
            "---\n",
            "\n",
            "## 5. Scaling & Production‑Ready Considerations\n",
            "\n",
            "| Concern | Strategies |\n",
            "|---------|------------|\n",
            "| **Latency** | • Use **approximate nearest neighbor (ANN)** indexes (IVF‑PQ, HNSW). <br>• Keep the vector store **in‑memory** or on a fast SSD. <br>• Batch embed queries when possible. |\n",
            "| **Throughput** | • Deploy the retriever as a **micro‑service** (FastAPI, gRPC). <br>• Use **vLLM** or **TGI** for high‑throughput LLM serving. |\n",
            "| **Context Window Limits** | • **Chunk selection**: rank by similarity, then truncate to fit the model’s token budget (e.g., 8 k for GPT‑4o). <br>• **Compression**: use summarization or extractive summarizer on top‑k docs before feeding. |\n",
            "| **Hybrid Retrieval** | Combine dense vectors with BM25 (lexical) to capture exact phrase matches. Use **reciprocal rank fusion** or a **learned reranker**. |\n",
            "| **Reranking** | After initial top‑k, run a cross‑encoder reranker (e.g., `cross‑encoder/ms‑marco-MiniLM-L-6-v2`) to improve relevance. |\n",
            "| **Incremental Indexing** | When new docs arrive, **append** chunks to the vector store; most stores support online upserts. |\n",
            "| **Security & Privacy** | • Keep **PII** out of the vector store (scrub or encrypt). <br>• Use **private embeddings** (self‑hosted models) if data cannot leave premises. |\n",
            "| **Observability** | Log: query, retrieved doc IDs, latency, LLM token usage, answer. Visualize with **Grafana** + **Prometheus** or **OpenTelemetry**. |\n",
            "| **Fail‑over** | If the LLM is unavailable, fall back to a **pure retrieval** answer (e.g., return top‑k passages). |\n",
            "| **Cost Management** | • Use cheaper embeddings for large corpora (e.g., `text-embedding-3-small`). <br>• Cache frequent queries and their results. |\n",
            "\n",
            "---\n",
            "\n",
            "## 6. Evaluation Metrics & Techniques\n",
            "\n",
            "| Metric | What it Measures | How to Compute |\n",
            "|--------|------------------|----------------|\n",
            "| **Recall@k** | Fraction of queries where a *relevant* passage appears in the top‑k retrieved set. | Use a labeled test set (e.g., Natural Questions). |\n",
            "| **Mean Reciprocal Rank (MRR)** | Average of reciprocal rank of the first relevant doc. | Same as above. |\n",
            "| **Precision@k** | Relevant docs among top‑k. | Useful when you limit context size. |\n",
            "| **F1 / ROUGE / BLEU** | Overlap between generated answer and a gold answer. | Standard NLP evaluation libraries. |\n",
            "| **Hallucination Score** | Proportion of statements not supported by any retrieved source. | Run a **fact‑checker** (e.g., `OpenAI GPT‑4` with “Is this statement supported by the sources?”). |\n",
            "| **Answer Faithfulness** | Whether the answer can be traced back to a citation. | Automatic citation detection + manual audit. |\n",
            "| **Latency & Throughput** | End‑to‑end response time, queries per second. | Load‑testing tools (Locust, k6). |\n",
            "| **Cost per Query** | Tokens used for embeddings + LLM calls. | Multiply token counts by provider pricing. |\n",
            "\n",
            "**Benchmark Datasets** (pick based on domain):  \n",
            "- **Open‑Domain QA**: Natural Questions, TriviaQA, WebQuestions.  \n",
            "- **Scientific**: SciFact, PubMedQA.  \n",
            "- **Legal**: CaseLaw QA (custom).  \n",
            "- **Code**: CodeSearchNet, MBPP (for code‑RAG).  \n",
            "\n",
            "**Human Evaluation**:  \n",
            "- **Relevance** (Does the answer address the question?).  \n",
            "- **Correctness** (Is the factual content accurate?).  \n",
            "- **Clarity** (Is the answer well‑written?).  \n",
            "- **Citation Quality** (Are citations correct and useful?).  \n",
            "\n",
            "---\n",
            "\n",
            "## 7. Advanced Topics & Extensions\n",
            "\n",
            "| Topic | Description & Tips |\n",
            "|-------|--------------------|\n",
            "| **Multi‑Hop Retrieval** | For complex questions, retrieve, generate an intermediate answer, then use that answer as a new query to fetch additional context. Implement with a loop or a “self‑ask” chain. |\n",
            "| **Dynamic Retrieval** | Use **LLM‑driven tool calling**: the model decides *when* to call the retriever (e.g., via LangChain’s `AgentExecutor`). |\n",
            "| **Structured Retrieval** | Retrieve from **tables** or **SQL**: embed rows, or use **SQL‑to‑NL** translation + execution. Tools: `LlamaIndex`’s `SQLTableNodeParser`. |\n",
            "| **Cross‑Modal RAG** | Combine text with images, audio, or video embeddings (e.g., CLIP for images). The prompt can include base64‑encoded image captions. |\n",
            "| **Feedback Loops** | Store user‑rated answers → re‑rank or fine‑tune a reranker. Use **RLHF** on the generator for domain‑specific style. |\n",
            "| **Prompt‑Tuning / Adapter Tuning** | Instead of full fine‑tuning, train a small prompt or LoRA adapter that biases the LLM toward using citations. |\n",
            "| **Self‑Consistency** | Sample the LLM multiple times, aggregate answers (majority vote) to reduce variance. |\n",
            "| **Tool‑Use Integration** | After generation, if the answer requires an action (e.g., “book a flight”), the LLM can invoke external APIs via LangChain tools. |\n",
            "| **Privacy‑Preserving Retrieval** | Use **Encrypted Search** (e.g., SEAL) or **Federated Retrieval** where embeddings stay on‑device and only similarity scores are exchanged. |\n",
            "| **Evaluation with LLM Judges** | Use a separate LLM (e.g., GPT‑4) to score answer quality, especially for open‑ended tasks. |\n",
            "\n",
            "---\n",
            "\n",
            "## 8. Recommended Toolkits & Ecosystem\n",
            "\n",
            "| Toolkit | Strengths | Typical Use‑Case |\n",
            "|---------|-----------|------------------|\n",
            "| **LangChain** | Modular chains, prompt templates, built‑in retrievers, agents. | Rapid prototyping, custom pipelines, tool‑use. |\n",
            "| **Haystack** | End‑to‑end pipelines, UI (Haystack UI), strong support for Elasticsearch & Milvus. | Enterprise QA, searchable docs, production‑grade services. |\n",
            "| **LlamaIndex (GPT Index)** | Tree‑structured indices (list, keyword, vector, summary), easy to load from many data sources. | Knowledge‑base bots, hierarchical retrieval. |\n",
            "| **Vercel AI SDK** | Serverless‑first, easy deployment on Vercel Edge Functions. | Low‑traffic web chat widgets. |\n",
            "| **vLLM / Text Generation Inference (TGI)** | High‑throughput serving of open‑source LLMs on GPUs. | Self‑hosted LLM back‑ends. |\n",
            "| **Pinecone / Weaviate / Qdrant** | Managed vector DB with scaling, metadata filtering, hybrid search. | SaaS‑style production. |\n",
            "| **FAISS / Annoy / HNSWLIB** | Free, in‑memory, great for research or small‑scale apps. | Prototyping, on‑device retrieval. |\n",
            "| **Guardrails / LLM‑Guard** | Runtime safety, schema enforcement, output validation. | Preventing toxic or out‑of‑scope answers. |\n",
            "| **OpenAI / Anthropic / Cohere SDKs** | Hosted LLMs with built‑in moderation, easy API keys. | Quick MVPs without GPU infra. |\n",
            "\n",
            "---\n",
            "\n",
            "## 9. End‑to‑End Deployment Blueprint\n",
            "\n",
            "1. **Data Pipeline (ETL)**  \n",
            "   - Cron job / Airflow DAG → fetch new docs → clean → chunk → embed → upsert into vector store.  \n",
            "\n",
            "2. **API Layer**  \n",
            "   - FastAPI / Flask / Node.js endpoint `/chat` that receives `{question}`.  \n",
            "   - Calls the **retriever service** → gets top‑k docs.  \n",
            "   - Calls **prompt service** → builds prompt.  \n",
            "   - Calls **LLM service** (OpenAI, vLLM, etc.).  \n",
            "   - Returns `{answer, citations, latency}`.  \n",
            "\n",
            "3. **Containerization**  \n",
            "   - Dockerfile for each micro‑service (retriever, generator).  \n",
            "   - Use **docker‑compose** for local dev; **Kubernetes** (Helm chart) for production.  \n",
            "\n",
            "4. **Observability Stack**  \n",
            "   - **OpenTelemetry** instrumentation in each service.  \n",
            "   - Export traces to **Jaeger**; metrics to **Prometheus**; dashboards in **Grafana**.  \n",
            "\n",
            "5. **CI/CD**  \n",
            "   - Lint + unit tests (pytest).  \n",
            "   - Integration test that runs a sample query against a staging index.  \n",
            "   - Deploy via GitHub Actions → Kubernetes rolling update.  \n",
            "\n",
            "6. **Security**  \n",
            "   - Store API keys in **Vault** / **AWS Secrets Manager**.  \n",
            "   - Enforce **HTTPS** and **OAuth2** for client authentication.  \n",
            "   - Run vector store in a private VPC; enable **IP‑allowlist** for the LLM API.  \n",
            "\n",
            "---\n",
            "\n",
            "## 10. Common Pitfalls & How to Avoid Them\n",
            "\n",
            "| Pitfall | Symptoms | Mitigation |\n",
            "|---------|----------|------------|\n",
            "| **Prompt exceeds context window** | Truncated sources → missing citations, incoherent answer. | Dynamically prune retrieved chunks by relevance score; use summarization; keep a token budget calculator. |\n",
            "| **Retriever returns irrelevant docs** | Low factuality, hallucinations. | Add a **reranker**, tune chunk size, improve embedding model, incorporate BM25. |\n",
            "| **Hallucinated facts not in sources** | Answer contains unsupported statements. | Enforce “answer only using sources” in system prompt; add a **post‑generation fact‑check** step. |\n",
            "| **Embedding drift** (new docs use a different embedding model) | Inconsistent similarity scores. | Version embeddings; re‑index old docs when you upgrade the model. |\n",
            "| **Latency spikes** | Users see >5 s response times. | Pre‑warm LLM workers, cache frequent queries, use ANN indexes, scale vector store horizontally. |\n",
            "| **Data leakage / PII** | Sensitive info appears in generated answer. | Scrub PII before indexing; use **private embeddings**; add a moderation filter after generation. |\n",
            "| **Cost blow‑out** | Unexpected high token usage. | Set `max_tokens` and `temperature=0`; monitor token usage; switch to cheaper embedding model for large corpora. |\n",
            "| **Evaluation bias** | Over‑optimistic metrics because test set overlaps with training data. | Keep a strict hold‑out set; use cross‑domain benchmarks. |\n",
            "\n",
            "---\n",
            "\n",
            "## 11. Future Directions (What to Watch)\n",
            "\n",
            "| Trend | Impact on RAG |\n",
            "|-------|----------------|\n",
            "| **Retrieval‑Augmented Generation with Retrieval‑Fine‑Tuned LLMs** (e.g., **RAG‑Fusion**, **Atlas**) – LLMs are trained to *jointly* retrieve and generate, reducing prompt engineering. |\n",
            "| **Multimodal Retrieval** – Embedding images, audio, and video alongside text; LLMs that can ingest those modalities (e.g., **GPT‑4V**, **LLaVA**). |\n",
            "| **Neural Re‑ranking at Scale** – Efficient cross‑encoders (e.g., **ColBERTv2**, **MiniLM‑Reranker**) that run on GPUs for billions of vectors. |\n",
            "| **LLM‑Based Index Construction** – Using LLMs to create hierarchical or graph‑based indices automatically. |\n",
            "| **Privacy‑Preserving Retrieval** – Homomorphic encryption or secure enclaves for on‑prem retrieval without exposing raw embeddings. |\n",
            "| **Tool‑Calling Agents** – RAG becomes a sub‑tool in larger autonomous agents that can plan, retrieve, act, and iterate. |\n",
            "| **Self‑Supervised Retrieval** – Models that learn to retrieve from their own generated data, reducing dependence on external corpora. |\n",
            "\n",
            "---\n",
            "\n",
            "## 12. TL;DR – Quick Checklist for a Production RAG System\n",
            "\n",
            "1. **Collect & Clean** data → chunk (200‑400 tokens).  \n",
            "2. **Embed** each chunk with a high‑quality model; store in an ANN vector DB (FAISS, Milvus, Pinecone).  \n",
            "3. **Add** a lexical BM25 index for hybrid search (optional but recommended).  \n",
            "4. **Retrieve** top‑k (k≈5‑10) chunks per query; optionally rerank.  \n",
            "5. **Build** a prompt that forces the LLM to cite sources; keep total tokens ≤ model’s context window.  \n",
            "6. **Generate** with a deterministic LLM (temperature≈0) for factual QA; higher temperature for creative tasks.  \n",
            "7. **Post‑process**: extract citations, run a fact‑check, filter profanity.  \n",
            "8. **Monitor** latency, token usage, relevance metrics; set alerts for drift.  \n",
            "9. **Scale**: ANN index, GPU‑accelerated LLM serving, caching, async pipelines.  \n",
            "10. **Secure**: scrub PII, keep keys secret, enforce access controls.  \n",
            "\n",
            "---\n",
            "\n",
            "### Final Thought\n",
            "\n",
            "RAG is **the bridge** between the *knowledge* stored in your documents and the *generative* capabilities of modern LLMs. By carefully engineering each piece—**data ingestion → embedding → retrieval → prompt → generation**—you can build a system that is **accurate, up‑to‑date, cost‑effective, and extensible** across domains ranging from customer support to scientific research. The code snippet above gets you started; the tables, best‑practice notes, and scaling guidelines will help you evolve that prototype into a robust production service. Happy building!\n",
            "--------------------------------------------------------------------------------\n",
            "Query: What are different ways to implement retrieval?\n",
            "Retriever Used: multi_query\n",
            "Answer: **Retrieval** is the step in a Retrieval‑Augmented Generation (RAG) pipeline that takes a user’s question (or any query) and returns the most relevant pieces of text from a knowledge base.  There are many ways to build a retriever, ranging from the simplest keyword‑based lookup to sophisticated, learned, multi‑stage pipelines.  Below is a comprehensive overview of the main families of retrieval techniques, together with the concrete patterns that LangChain (and LangSmith) surface in its tutorials.\n",
            "\n",
            "---\n",
            "\n",
            "## 1. Pure **Keyword / Sparse** Retrieval  \n",
            "\n",
            "| Technique | How it works | Typical tools | When to use |\n",
            "|-----------|--------------|---------------|-------------|\n",
            "| **BM25 / TF‑IDF** | Scores documents by term frequency and inverse document frequency; pure lexical match. | Elasticsearch, OpenSearch, Whoosh, Lucene, PostgreSQL full‑text search | Small‑to‑medium corpora, when exact phrase matching or Boolean filters are critical, or when you need deterministic results. |\n",
            "| **Keyword‑only vector stores** (e.g., **FAISS with sparse embeddings**) | Stores sparse vectors (e.g., from SPLADE, SPECTER‑sparse) that preserve term information while still enabling fast ANN search. | FAISS, Annoy, Milvus (sparse mode) | When you want the speed of ANN but still want lexical bias. |\n",
            "\n",
            "**Pros:** Fast, deterministic, easy to filter with Boolean logic.  \n",
            "**Cons:** Misses semantic similarity (e.g., “car” vs. “automobile”).\n",
            "\n",
            "---\n",
            "\n",
            "## 2. Pure **Dense / Semantic** Retrieval  \n",
            "\n",
            "| Technique | How it works | Typical models | Typical vector stores |\n",
            "|-----------|--------------|----------------|-----------------------|\n",
            "| **Single‑embedding per document (or per chunk)** | Encode each text chunk with a transformer encoder (e.g., OpenAI `text-embedding-ada-002`, `sentence‑transformers`, `Cohere`, `HuggingFace` models). Store the resulting dense vector. | OpenAI, Cohere, HuggingFace sentence‑transformers, Mistral‑embed, etc. | FAISS, Pinecone, Weaviate, Qdrant, Milvus, Chroma, etc. |\n",
            "| **Multiple embeddings per document** | Create several embeddings that capture different aspects (e.g., title vs. body, different prompts, or multi‑modal embeddings). | Same models, but run multiple prompts or use multi‑modal encoders (e.g., CLIP for image+text). | Same stores; you may keep separate namespaces or concatenate vectors. |\n",
            "| **Hybrid dense‑sparse** (dense + sparse vectors concatenated) | Combine a dense embedding with a sparse lexical embedding to get the best of both worlds. | SPLADE‑dense, ColBERT‑v2, or simply concatenate OpenAI dense + BM25 sparse. | Stores that support concatenated vectors (FAISS, Milvus). |\n",
            "\n",
            "**Pros:** Captures semantic similarity, robust to paraphrase.  \n",
            "**Cons:** Requires an embedding model, may ignore exact term constraints, needs periodic re‑embedding when data changes.\n",
            "\n",
            "---\n",
            "\n",
            "## 3. **Hybrid Retrieval** (Dense + Keyword)\n",
            "\n",
            "1. **Two‑stage retrieval** – first run a fast dense ANN to get a candidate set (e.g., top‑k = 100), then re‑rank those candidates with a lexical scorer (BM25) or a cross‑encoder.  \n",
            "2. **Score fusion** – compute a weighted sum of the dense similarity score and the sparse BM25 score for each document, then sort.  \n",
            "3. **Hybrid vector stores** – some stores (e.g., **Weaviate**, **Qdrant**) natively support hybrid queries that combine vector similarity with property/keyword filters.\n",
            "\n",
            "**Why use it?**  \n",
            "Hybrid retrieval often yields higher recall than either dense or sparse alone, especially on mixed corpora where some queries are highly lexical (e.g., code identifiers) and others are conceptual.\n",
            "\n",
            "---\n",
            "\n",
            "## 4. **Retriever Enhancements & Variations**\n",
            "\n",
            "| Enhancement | What it adds | Typical implementation |\n",
            "|-------------|--------------|------------------------|\n",
            "| **Query rewriting / expansion** | A language model rewrites the user query into a more effective search query (e.g., adds synonyms, removes noise, splits multi‑intent queries). | `LLMChain` that calls an LLM with a prompt like “Rewrite the following question to be concise and keyword‑rich.” |\n",
            "| **Multiple query generation** | Instead of a single query, generate several alternative queries (e.g., “What is the capital of France?” → “Paris capital”, “France main city”). Retrieve with each and union the results. | `MultiQueryRetriever` in LangChain. |\n",
            "| **Contextual compression** | After retrieving a large set of chunks, a compressor (often a cross‑encoder or LLM) selects the most relevant subset to pass to the LLM, reducing token usage. | `ContextualCompressionRetriever`. |\n",
            "| **Metadata filters** | Attach structured metadata (date, author, source, tags) to each vector and filter at retrieval time (e.g., “documents since 2020”). | Vector store filters (`metadata={\"year\": {\"$gte\": 2020}}`). |\n",
            "| **Time‑weighted / recency bias** | Boost scores of newer documents (or decay older ones) to surface fresher information. | Custom scoring function that adds a decay term based on `timestamp`. |\n",
            "| **Custom retriever class** | Implement any bespoke logic (e.g., combine external APIs, enforce business rules). | Subclass `BaseRetriever` in LangChain and override `get_relevant_documents`. |\n",
            "| **Similarity scores exposure** | Return the raw similarity score alongside each document so downstream logic can threshold or rank further. | Set `return_score=True` in the vector store query. |\n",
            "| **Combining multiple retrievers** | Run several independent retrievers (e.g., dense, keyword, graph‑based) and merge/union their results, possibly with deduplication. | `EnsembleRetriever` or manual union + re‑ranking. |\n",
            "| **Re‑ordering to mitigate “lost in the middle”** | After retrieval, reorder results to surface the most informative chunks (e.g., move the most central chunk to the top). | Simple heuristics (move highest‑score chunk first) or LLM‑based re‑ranking. |\n",
            "| **Retrieving whole documents for a chunk** | When a chunk is selected, fetch the full source document (or surrounding context) to give the LLM more continuity. | Store a `doc_id` with each chunk; after retrieval, load the full doc from storage. |\n",
            "| **Multi‑modal retrieval** | Retrieve not only text but also images, audio, or tables using embeddings from CLIP, Whisper, etc. | Store multi‑modal vectors in the same store, query with the appropriate modality. |\n",
            "\n",
            "---\n",
            "\n",
            "## 5. **Retrieval Architectures in Practice**\n",
            "\n",
            "1. **Simple vector‑store retriever** – One vector store, single embedding per chunk, top‑k ANN.  \n",
            "   *Typical code:* `retriever = PineconeVectorStore(...).as_retriever(search_kwargs={\"k\": 5})`\n",
            "\n",
            "2. **Hybrid retriever pipeline** – Dense ANN → lexical re‑rank → filter → compression.  \n",
            "   *Typical flow:*  \n",
            "   ```python\n",
            "   dense_hits = dense_store.as_retriever(k=100).get_relevant_documents(query)\n",
            "   lexical_hits = bm25_index.search(query, k=100)\n",
            "   combined = fuse(dense_hits, lexical_hits, alpha=0.6)\n",
            "   top_n = rank_with_cross_encoder(combined, query, k=10)\n",
            "   final = compress_with_llm(top_n, query, max_tokens=1500)\n",
            "   ```\n",
            "\n",
            "3. **Multi‑query + ensemble** – Generate N query variants, retrieve with each, union, then optionally re‑rank.  \n",
            "   *Typical code:* `retriever = MultiQueryRetriever(base_retriever, llm=OpenAI())`\n",
            "\n",
            "4. **Time‑weighted retriever** – Add a recency boost to the similarity score before ranking.  \n",
            "   *Pseudo‑score:* `final_score = dense_score + λ * exp(-Δt / τ)`\n",
            "\n",
            "5. **Custom retriever** – For domain‑specific logic (e.g., “only return documents that have a `status:approved` flag and are from the `finance` department`).  \n",
            "   *Implementation:* subclass `BaseRetriever` and embed the business rules in `get_relevant_documents`.\n",
            "\n",
            "---\n",
            "\n",
            "## 6. **Choosing the Right Retrieval Strategy**\n",
            "\n",
            "| Situation | Recommended approach |\n",
            "|-----------|----------------------|\n",
            "| **Small, static corpus (<10k docs) & exact phrase matching needed** | Pure BM25 / keyword search with filters. |\n",
            "| **Medium‑size corpus (10k‑100k) with semantic queries** | Dense vector store (single embedding per chunk) + optional metadata filters. |\n",
            "| **Large, heterogeneous corpus (mix of code, legal text, FAQs)** | Hybrid dense + sparse retrieval + query rewriting. |\n",
            "| **Need to keep results fresh (news, logs)** | Time‑weighted retriever + metadata date filter. |\n",
            "| **User queries often contain multiple intents** | Multi‑query generation + union of results. |\n",
            "| **Token budget is tight for the downstream LLM** | Contextual compression after retrieval. |\n",
            "| **Business rules dictate strict inclusion/exclusion** | Custom retriever class with explicit filters. |\n",
            "| **You want to experiment quickly** | Use LangChain’s built‑in `VectorStoreRetriever`, `MultiQueryRetriever`, `EnsembleRetriever` – they already implement many of the patterns above. |\n",
            "\n",
            "---\n",
            "\n",
            "## 7. **Putting It All Together – A Sample LangChain Pipeline**\n",
            "\n",
            "```python\n",
            "from langchain.vectorstores import Pinecone\n",
            "from langchain.retrievers import MultiQueryRetriever, ContextualCompressionRetriever\n",
            "from langchain.llms import OpenAI\n",
            "from langchain.embeddings import OpenAIEmbeddings\n",
            "from langchain.retrievers import EnsembleRetriever\n",
            "\n",
            "# 1️⃣ Dense vector store\n",
            "dense_store = Pinecone.from_texts(\n",
            "    texts, OpenAIEmbeddings(), index_name=\"my-index\"\n",
            ")\n",
            "\n",
            "# 2️⃣ Keyword (BM25) retriever – using a simple in‑memory index for demo\n",
            "keyword_retriever = BM25Retriever.from_texts(texts)\n",
            "\n",
            "# 3️⃣ Multi‑query generation (LLM rewrites)\n",
            "multi_query = MultiQueryRetriever(\n",
            "    base_retriever=dense_store.as_retriever(k=20),\n",
            "    llm=OpenAI(model=\"gpt-4o-mini\")\n",
            ")\n",
            "\n",
            "# 4️⃣ Ensemble (dense + keyword) + time‑weight boost\n",
            "ensemble = EnsembleRetriever(\n",
            "    retrievers=[multi_query, keyword_retriever],\n",
            "    weights=[0.7, 0.3],\n",
            "    combine_method=\"score_fusion\"\n",
            ")\n",
            "\n",
            "# 5️⃣ Contextual compression (cross‑encoder)\n",
            "compressor = ContextualCompressionRetriever(\n",
            "    base_retriever=ensemble,\n",
            "    compressor=OpenAIEmbeddings(model=\"text-embedding-3-large\")  # or a cross‑encoder\n",
            ")\n",
            "\n",
            "# 6️⃣ Use in a RAG chain\n",
            "qa_chain = RetrievalQA.from_chain_type(\n",
            "    llm=OpenAI(),\n",
            "    retriever=compressor,\n",
            "    return_source_documents=True\n",
            ")\n",
            "\n",
            "answer = qa_chain.run(\"What are the latest privacy‑policy changes for 2024?\")\n",
            "print(answer[\"answer\"])\n",
            "print(\"Sources:\", [doc.metadata[\"source\"] for doc in answer[\"source_documents\"]])\n",
            "```\n",
            "\n",
            "This example demonstrates **multiple retrieval ideas in one pipeline**:\n",
            "- dense semantic search,\n",
            "- keyword fallback,\n",
            "- query expansion,\n",
            "- score fusion,\n",
            "- time‑aware weighting (could be added in a custom scorer),\n",
            "- and final compression before feeding the LLM.\n",
            "\n",
            "---\n",
            "\n",
            "## 8. **Key Take‑aways**\n",
            "\n",
            "1. **Retrieval is not a single technique** – it can be lexical, dense, hybrid, or a combination of many sub‑components.  \n",
            "2. **LangChain (and LangSmith) expose most of these patterns as ready‑made retriever classes** (`VectorStoreRetriever`, `MultiQueryRetriever`, `ContextualCompressionRetriever`, `EnsembleRetriever`, etc.).  \n",
            "3. **Metadata and filters** give you deterministic control (date, author, tags).  \n",
            "4. **Query rewriting and multi‑query generation** help the retriever understand ambiguous or multi‑facet questions.  \n",
            "5. **Post‑retrieval compression** is essential when you need to stay within LLM token limits while preserving the most relevant information.  \n",
            "6. **Custom retrievers** let you embed any business logic that isn’t covered by the built‑ins.\n",
            "\n",
            "By mixing and matching these building blocks you can craft a retrieval layer that is **fast, accurate, and aligned with the specific constraints of your application**—whether you need pure semantic search, strict keyword compliance, recency bias, or a sophisticated multi‑stage pipeline.\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/langchain_core/vectorstores/base.py:1083: UserWarning: Relevance scores must be between 0 and 1, got [(Document(id='7291d80d-a709-4c0b-a0df-711b6ade3555', metadata={'language': 'en', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App: Part 1 | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain'}, page_content=\"Load: First we need to load our data. This is done with Document Loaders.\\nSplit: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won't fit in a model's finite context window.\\nStore: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\\n\\n\\nRetrieval and generation‚Äã\\n\\nRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\\nGenerate: A ChatModel / LLM produces an answer using a prompt that includes both the question with the retrieved data\"), -0.015077849353107187), (Document(id='386e676a-ba4d-41a4-b31f-ae1406426f8d', metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App: Part 1 | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='Learn more about splitting text using different methods by reading the how-to docs\\nCode (py or js)\\nScientific papers\\nInterface: API reference for the base interface.\\n\\nDocumentTransformer: Object that performs a transformation on a list\\nof Document objects.\\n\\nDocs: Detailed documentation on how to use DocumentTransformers\\nIntegrations\\nInterface: API reference for the base interface.'), -0.07370790884812672), (Document(id='ad847403-037a-42eb-9de8-9f70716d7c00', metadata={'language': 'en', 'title': 'Build a Retrieval Augmented Generation (RAG) App: Part 1 | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'source': 'https://python.langchain.com/docs/tutorials/rag/', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.'}, page_content=\"We'll use LangGraph to tie together the retrieval and generation steps into a single application. This will bring a number of benefits:\"), -0.07650637529555304)]\n",
            "  self.vectorstore.similarity_search_with_relevance_scores(\n",
            "WARNING:langchain_core.vectorstores.base:No relevant docs were retrieved using the relevance score threshold 0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: Show me diverse approaches to document processing\n",
            "Retriever Used: self_query\n",
            "Answer: Below is a **catalog of the most common (and a few emerging) ways to process documents**, grouped by the stage of the pipeline they belong to and the type of technique they use.  \n",
            "Feel free to cherry‑pick the pieces that fit your use‑case, mix‑and‑match them, or use the whole end‑to‑end flow as a starter template.\n",
            "\n",
            "---\n",
            "\n",
            "## 1️⃣  Pre‑ingestion – Getting the raw material into a usable form  \n",
            "\n",
            "| Approach | What it does | Typical tools / libraries | When to use it |\n",
            "|----------|--------------|---------------------------|----------------|\n",
            "| **File‑type parsers** | Convert PDFs, DOCX, HTML, emails, scans (OCR) → plain text or structured JSON | `pdfminer`, `PyMuPDF`, `docx2txt`, `BeautifulSoup`, `pdfplumber`, `tesseract‑ocr`, `unstructured.io` | You have heterogeneous sources; you need a single “text” representation. |\n",
            "| **Language detection & normalization** | Detect language, transliterate, normalize Unicode, strip diacritics | `langdetect`, `fasttext` language ID, `unicodedata.normalize` | Multi‑lingual corpora or noisy scraped data. |\n",
            "| **Noise removal / cleaning** | Strip boilerplate, headers/footers, HTML tags, code blocks, tables | `boilerpy3`, custom regex, `readability‑lxml` | Web‑scraped articles, logs, or any content with repetitive UI noise. |\n",
            "| **Metadata extraction** | Pull author, date, source URL, tags, section headings, etc. | `extruct`, `pdfinfo`, custom regex, `metadata‑extractor` | You need to filter or rank later on (e.g., time‑weighted retrieval). |\n",
            "| **Chunk‑level indexing** | Keep track of the original start/end character offsets, page numbers, or section IDs | LangChain `add_start_index=True`, custom `Document` objects | Later you may want to “jump back” to the original location for display or citation. |\n",
            "\n",
            "---\n",
            "\n",
            "## 2️⃣  Text‑level transformation – Turning raw strings into richer representations  \n",
            "\n",
            "| Category | Technique | What you get | Tools / models |\n",
            "|----------|-----------|--------------|----------------|\n",
            "| **Tokenization & normalization** | Word‑piece, byte‑pair, whitespace, sentence tokenizers; lower‑casing, stemming, lemmatization | Token list + optional POS tags | `spaCy`, `NLTK`, `Stanza`, `sentence‑piece` |\n",
            "| **Stop‑word / noise filtering** | Remove high‑frequency function words, numbers, URLs, email addresses | Smaller, more signal‑rich token set | `spaCy` stop‑word list, custom regex |\n",
            "| **Named‑entity & concept extraction** | Detect PERSON, ORG, DATE, product names, medical codes, etc. | Structured entities (type, span, confidence) | `spaCy`, `flair`, `HuggingFace` NER models, `scispaCy` |\n",
            "| **Keyphrase / topic extraction** | RAKE, YAKE, TextRank, BERTopic, LDA, NMF | List of salient phrases or topics per doc | `keybert`, `bertopic`, `gensim` |\n",
            "| **Semantic embedding** | Map a chunk → dense vector (usually 384‑1536 dims) that captures meaning | Vector ready for similarity search | `OpenAI text‑embedding‑ada‑002`, `sentence‑transformers`, `Cohere`, `HuggingFace` models |\n",
            "| **Multi‑embedding per doc** | Produce several vectors per chunk (e.g., one for content, one for metadata, one for title) | Enables hybrid scoring (content + metadata) | Custom pipeline – store each vector with a tag. |\n",
            "| **Document‑level summarisation** | Condense long text into a short abstract (extractive or abstractive) | Summary string (often stored as a separate field) | `OpenAI gpt‑3.5‑turbo`, `bart‑large‑cnn`, `llama‑2‑7b‑chat` |\n",
            "| **Text‑to‑graph conversion** | Build a knowledge graph from entities & relations | Nodes/edges that can be traversed for reasoning | `Neo4j`, `rdflib`, `langchain‑graph` |\n",
            "\n",
            "---\n",
            "\n",
            "## 3️⃣  Chunking – Splitting a document into retrieval‑friendly pieces  \n",
            "\n",
            "| Method | How it works | Typical parameters | When it shines |\n",
            "|--------|--------------|--------------------|----------------|\n",
            "| **RecursiveCharacterTextSplitter** (LangChain) | Recursively split on a hierarchy of separators (e.g., `\\n\\n`, `\\n`, ` `) until each piece fits `chunk_size` with optional `chunk_overlap` | `chunk_size=1000`, `chunk_overlap=200` | Generic text, balanced between context and vector‑store limits. |\n",
            "| **Semantic chunking** | Use a sliding‑window of embeddings; cut where similarity drops below a threshold | `window=5 sentences`, `similarity_thresh=0.6` | Long narratives where topic shifts are more meaningful than line breaks. |\n",
            "| **Sentence‑level splitter** | One sentence per chunk (or a few sentences) | `max_sentences=3` | Retrieval for QA where you want the smallest possible context. |\n",
            "| **HTML/Markdown structural splitter** | Split on headings (`<h1>`, `##`) preserving hierarchy | `keep_headers=True` | Technical docs, manuals, or any markup‑rich content. |\n",
            "| **Hybrid splitter** | Combine a structural split first, then apply a character‑based split inside each section | `section_splitter` + `RecursiveCharacterTextSplitter` | Long reports with clear sections (e.g., “Methods”, “Results”). |\n",
            "| **Time‑weighted splitter** | Attach a decay factor to chunks based on document timestamp (used later for scoring) | `decay=0.9 per month` | News feeds, logs, or any data where recency matters. |\n",
            "\n",
            "**Tip:** Always store `add_start_index=True` (or an equivalent) so you can reconstruct the original location when you later present a result.\n",
            "\n",
            "---\n",
            "\n",
            "## 4️⃣  Indexing / Vector Store – Making the chunks searchable  \n",
            "\n",
            "| Vector store | Key features | When to pick it |\n",
            "|--------------|--------------|-----------------|\n",
            "| **FAISS (Flat, IVF, HNSW)** | Open‑source, GPU/CPU, supports IVF‑PQ for compression, custom metric | Large‑scale (> 10 M) embeddings, on‑prem. |\n",
            "| **Pinecone** | Managed, automatic scaling, metadata filtering, hybrid (vector + keyword) | Production SaaS, need low‑latency and easy ops. |\n",
            "| **Weaviate** | Graph‑aware, built‑in modules for BM25 + vector, schema‑driven, supports hybrid search | When you want to query both semantic and relational data. |\n",
            "| **Qdrant** | Open‑source + managed, supports payload filters, payload‑indexed vectors | Need payload‑level filters (e.g., “author=John”) with fast recall. |\n",
            "| **Chroma** | Simple Python‑first API, good for prototyping, supports collection‑level metadata | Small‑to‑medium projects, notebooks, rapid iteration. |\n",
            "| **Milvus** | Distributed, supports IVF, HNSW, scalar filters, GPU acceleration | Very large clusters, multi‑tenant environments. |\n",
            "\n",
            "**Common indexing steps**\n",
            "\n",
            "```python\n",
            "# 1️⃣  Create documents → split → embed\n",
            "docs = loader.load()                     # any DocumentLoader\n",
            "splits = RecursiveCharacterTextSplitter(\n",
            "    chunk_size=1000, chunk_overlap=200,\n",
            "    add_start_index=True\n",
            ").split_documents(docs)\n",
            "\n",
            "embeddings = OpenAIEmbeddings()          # or SentenceTransformer(...)\n",
            "vectors = embeddings.embed_documents([s.page_content for s in splits])\n",
            "\n",
            "# 2️⃣  Build the store (example with Pinecone)\n",
            "import pinecone\n",
            "pinecone.init(api_key=\"YOUR_KEY\", environment=\"us-west1-gcp\")\n",
            "index = pinecone.Index(\"my-doc-index\")\n",
            "index.upsert(vectors=[(str(i), vec, {\"source\": s.metadata[\"source\"],\n",
            "                                    \"start_idx\": s.metadata[\"start_index\"]})\n",
            "                     for i, (s, vec) in enumerate(zip(splits, vectors))])\n",
            "```\n",
            "\n",
            "---\n",
            "\n",
            "## 5️⃣  Retrieval – Pulling the most relevant chunks  \n",
            "\n",
            "| Retrieval style | How it works | Typical use‑case |\n",
            "|-----------------|--------------|-----------------|\n",
            "| **Pure vector similarity** | Cosine / inner‑product nearest‑neighbors | Semantic QA, “find similar ideas”. |\n",
            "| **Hybrid vector + keyword (BM25)** | Combine a dense score with a sparse TF‑IDF/BM25 score (often via weighted sum) | When you need exact term matching *and* semantic matching (e.g., “Python *list* comprehension”). |\n",
            "| **Metadata‑filtered retrieval** | Apply a filter on payload fields before similarity search (e.g., `source=\"github.com\"` or `date > 2024‑01‑01`) | Time‑weighted, domain‑specific, or user‑access‑controlled queries. |\n",
            "| **Contextual compression** | Retrieve a *large* set (e.g., top‑50), then run a LLM‑based “compressor” that selects the most useful subset for the final prompt | Reduces token usage while preserving signal for long‑context LLMs. |\n",
            "| **Multi‑retriever ensemble** | Run several retrievers (e.g., vector, keyword, graph) and merge results (reciprocal rank fusion, weighted voting) | Improves recall on heterogeneous corpora. |\n",
            "| **Re‑ranking with LLM** | After an initial vector search, feed the top‑k chunks to a LLM that scores relevance to the query | When you need higher precision for a single answer. |\n",
            "| **Time‑weighted scoring** | Multiply the similarity score by a decay factor based on document age | News, logs, or any “freshness‑important” data. |\n",
            "| **Self‑query retriever** | The LLM generates *multiple* sub‑queries from the original question, runs each through the vector store, and merges the results | Complex questions that need multi‑facet evidence (e.g., “compare the 2022 and 2023 earnings and explain the trend”). |\n",
            "\n",
            "**Example: Hybrid retrieval with Pinecone**\n",
            "\n",
            "```python\n",
            "# 1️⃣  Sparse BM25 query (using a separate ElasticSearch or Pinecone's sparse module)\n",
            "sparse_query = {\"match\": {\"content\": \"machine learning\"}}\n",
            "\n",
            "# 2️⃣  Dense query\n",
            "dense_vec = embeddings.embed_query(\"What are the latest trends in machine learning?\")\n",
            "\n",
            "# 3️⃣  Hybrid search (Pinecone supports it out‑of‑the‑box)\n",
            "results = index.query(\n",
            "    vector=dense_vec,\n",
            "    top_k=10,\n",
            "    filter={\"source\": {\"$in\": [\"blog\", \"paper\"]}},\n",
            "    sparse_vector=sparse_query,\n",
            "    hybrid=True,               # tells Pinecone to blend scores\n",
            "    alpha=0.6                  # weight for dense vs. sparse (0‑1)\n",
            ")\n",
            "```\n",
            "\n",
            "---\n",
            "\n",
            "## 6️⃣  Post‑retrieval – Turning raw chunks into a final answer  \n",
            "\n",
            "| Step | What it does | Typical implementation |\n",
            "|------|--------------|------------------------|\n",
            "| **Rerank / relevance scoring** | LLM evaluates each candidate against the original query | `openai.ChatCompletion` with a “score this passage” prompt, or `cohere.rerank`. |\n",
            "| **Contextual compression** | LLM selects a subset that fits the token budget | `langchain.retrievers.ContextualCompressionRetriever`. |\n",
            "| **Prompt assembly** | Concatenate selected chunks (with optional citations) + system prompt | Use LangChain `PromptTemplate` or custom Jinja template. |\n",
            "| **Answer generation** | LLM produces the final answer, optionally citing sources | `gpt‑4‑turbo`, `Claude`, `Gemini`. |\n",
            "| **Citation formatting** | Convert `source` + `start_index` into a human‑readable reference (e.g., markdown footnote) | Simple string formatting or a citation library. |\n",
            "| **Self‑critique / verification** | Run a second LLM pass that checks the answer for hallucinations or missing citations | “Self‑Ask” pattern, or `langchain.chains.SelfCheckChain`. |\n",
            "\n",
            "**Sample prompt template**\n",
            "\n",
            "```text\n",
            "You are an expert research assistant. Use ONLY the provided excerpts to answer the question.\n",
            "If you need to quote, wrap the excerpt in triple backticks and add a footnote with its source.\n",
            "\n",
            "Question: {question}\n",
            "\n",
            "Context:\n",
            "{retrieved_chunks}\n",
            "\n",
            "Answer (with citations):\n",
            "```\n",
            "\n",
            "---\n",
            "\n",
            "## 7️⃣  Advanced / Emerging Techniques  \n",
            "\n",
            "| Technique | Why it matters | Example libraries / papers |\n",
            "|-----------|----------------|----------------------------|\n",
            "| **Retrieval‑augmented generation (RAG) with fine‑tuned retriever** | Jointly train the retriever to maximize downstream QA performance, often using contrastive loss. | `FAISS + DPR`, `ColBERTv2`, `MTEB` benchmarks. |\n",
            "| **Neural re‑ranking with cross‑encoders** | Encode *query + passage* together for a more accurate relevance score (costly but high‑precision). | `sentence‑transformers` `cross‑encoder/ms-marco-MiniLM-L-12-v2`. |\n",
            "| **Graph‑augmented retrieval** | Use a knowledge graph to expand queries or filter results (e.g., “find all docs mentioning a person and their affiliated org”). | `Neo4j + LangChain GraphRetriever`. |\n",
            "| **Multimodal retrieval** | Store image embeddings, audio spectrogram embeddings, or video key‑frame embeddings alongside text. | `CLIP`, `Whisper`, `FAISS` with mixed payloads. |\n",
            "| **Dynamic chunk sizing** | Adjust chunk length on‑the‑fly based on token budget of the downstream LLM (e.g., shrink if the LLM is GPT‑3.5 vs. GPT‑4). | Custom `DynamicChunkSplitter`. |\n",
            "| **Privacy‑preserving retrieval** | Use homomorphic encryption or secure enclaves so the vector store never sees raw text. | `Microsoft SEAL`, `Pinecone Private Endpoint`. |\n",
            "| **Self‑query generation** | The LLM creates a set of sub‑questions that explore different facets, then merges the answers. | LangChain `SelfQueryRetriever`. |\n",
            "| **Temporal vector stores** | Store multiple “snapshots” of embeddings over time and query the version that matches a given date. | `Weaviate` with `vector_timestamp` payload. |\n",
            "\n",
            "---\n",
            "\n",
            "## 8️⃣  Putting It All Together – A Minimal End‑to‑End Pipeline (Python)\n",
            "\n",
            "```python\n",
            "# -------------------------------------------------\n",
            "# 1️⃣  Load & clean\n",
            "# -------------------------------------------------\n",
            "from langchain.document_loaders import UnstructuredURLLoader\n",
            "loader = UnstructuredURLLoader(urls=[\"https://example.com/article\"])\n",
            "raw_docs = loader.load()\n",
            "\n",
            "# -------------------------------------------------\n",
            "# 2️⃣  Split\n",
            "# -------------------------------------------------\n",
            "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
            "splitter = RecursiveCharacterTextSplitter(\n",
            "    chunk_size=1000,\n",
            "    chunk_overlap=200,\n",
            "    add_start_index=True,\n",
            ")\n",
            "chunks = splitter.split_documents(raw_docs)\n",
            "\n",
            "# -------------------------------------------------\n",
            "# 3️⃣  Embed\n",
            "# -------------------------------------------------\n",
            "from langchain.embeddings import OpenAIEmbeddings\n",
            "embedder = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
            "vectors = embedder.embed_documents([c.page_content for c in chunks])\n",
            "\n",
            "# -------------------------------------------------\n",
            "# 4️⃣  Store (using Chroma for demo)\n",
            "# -------------------------------------------------\n",
            "from langchain.vectorstores import Chroma\n",
            "db = Chroma.from_documents(\n",
            "    documents=chunks,\n",
            "    embedding=embedder,\n",
            "    collection_name=\"my_corpus\"\n",
            ")\n",
            "\n",
            "# -------------------------------------------------\n",
            "# 5️⃣  Retrieve (hybrid + metadata filter)\n",
            "# -------------------------------------------------\n",
            "retriever = db.as_retriever(\n",
            "    search_type=\"mmr\",          # Max‑Marginal‑Relevance for diversity\n",
            "    search_kwargs={\"k\": 10}\n",
            ")\n",
            "\n",
            "# Optional: add a metadata filter (e.g., only recent docs)\n",
            "retriever.search_kwargs[\"filter\"] = {\"date\": {\"$gte\": \"2024-01-01\"}}\n",
            "\n",
            "# -------------------------------------------------\n",
            "# 6️⃣  Rerank with a cross‑encoder (optional)\n",
            "# -------------------------------------------------\n",
            "from sentence_transformers import CrossEncoder\n",
            "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-12-v2\")\n",
            "def rerank(query, docs, top_k=5):\n",
            "    scores = cross_encoder.predict([(query, d.page_content) for d in docs])\n",
            "    ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
            "    return [d for d, _ in ranked[:top_k]]\n",
            "\n",
            "# -------------------------------------------------\n",
            "# 7️⃣  Answer generation\n",
            "# -------------------------------------------------\n",
            "from langchain.llms import OpenAI\n",
            "from langchain.prompts import PromptTemplate\n",
            "llm = OpenAI(model=\"gpt-4-turbo\")\n",
            "template = PromptTemplate.from_template(\n",
            "\"\"\"You are a helpful assistant. Answer the question using ONLY the excerpts below.\n",
            "If you quote, add a footnote with the source URL.\n",
            "\n",
            "Question: {question}\n",
            "\n",
            "Context:\n",
            "{context}\n",
            "\"\"\"\n",
            ")\n",
            "\n",
            "def answer(question):\n",
            "    # 5️⃣ retrieve\n",
            "    docs = retriever.get_relevant_documents(question)\n",
            "    # 6️⃣ optional rerank\n",
            "    docs = rerank(question, docs, top_k=5)\n",
            "    # 7️⃣ build prompt\n",
            "    prompt = template.format(\n",
            "        question=question,\n",
            "        context=\"\\n\\n\".join([d.page_content for d in docs])\n",
            "    )\n",
            "    return llm(prompt)\n",
            "\n",
            "# -------------------------------------------------\n",
            "# 8️⃣  Run\n",
            "# -------------------------------------------------\n",
            "print(answer(\"What are the main challenges of using vector stores for legal documents?\"))\n",
            "```\n",
            "\n",
            "*The code above demonstrates a **complete flow**: ingestion → cleaning → chunking → embedding → vector store → hybrid retrieval → optional cross‑encoder rerank → LLM answer with citations.*\n",
            "\n",
            "---\n",
            "\n",
            "## 📚 Quick‑Reference Cheat‑Sheet\n",
            "\n",
            "| Stage | Key Choices | “Go‑to” Library |\n",
            "|-------|-------------|-----------------|\n",
            "| **Ingestion** | PDF/HTML/Email parsers, OCR | `unstructured.io`, `pdfplumber` |\n",
            "| **Cleaning** | Boilerplate removal, language detection | `boilerpy3`, `langdetect` |\n",
            "| **Chunking** | RecursiveCharacter, Semantic, Heading‑aware | `langchain_text_splitters` |\n",
            "| **Embedding** | OpenAI, Cohere, Sentence‑Transformers | `langchain.embeddings` |\n",
            "| **Vector Store** | FAISS, Pinecone, Weaviate, Qdrant, Chroma | `langchain.vectorstores` |\n",
            "| **Retrieval** | Pure vector, Hybrid, Metadata‑filter, Time‑weighted | `as_retriever`, `HybridSearchRetriever` |\n",
            "| **Rerank** | Cross‑encoder, LLM‑based | `sentence_transformers.CrossEncoder` |\n",
            "| **Compression** | ContextualCompressionRetriever | `langchain.retrievers` |\n",
            "| **Answer Generation** | PromptTemplate + LLM | `langchain.llms` |\n",
            "| **Citation** | Store `source` + `start_index` → footnotes | Custom formatter |\n",
            "\n",
            "---\n",
            "\n",
            "### TL;DR\n",
            "\n",
            "1. **Parse & clean** raw files → plain text + metadata.  \n",
            "2. **Split** into overlapping chunks (RecursiveCharacter is a solid default).  \n",
            "3. **Embed** each chunk with a dense model (OpenAI ada‑002 or SBERT).  \n",
            "4. **Index** the vectors in a store that supports metadata filters (FAISS, Pinecone, Weaviate, Qdrant, Chroma).  \n",
            "5. **Retrieve** using a hybrid of dense similarity + optional keyword/BM25 + metadata constraints.  \n",
            "6. **Optionally rerank/compress** the top‑k results with a cross‑encoder or LLM.  \n",
            "7. **Prompt** the LLM with the selected excerpts and a citation‑aware template.  \n",
            "8. **Generate** the final answer, add footnotes, and (if needed) run a self‑critique pass.\n",
            "\n",
            "Mix‑and‑match any of the rows above to suit your domain (legal, medical, code, news, etc.) and you’ll have a **robust, diverse document‑processing pipeline** ready for production or research. Happy building!\n",
            "--------------------------------------------------------------------------------\n",
            "Query: What if I wanted to create hypothetical documents?\n",
            "Retriever Used: hyde\n",
            "Answer: ### Short answer  \n",
            "Yes – you can create **hypothetical (synthetic) documents** and feed them into the Hyde retriever for testing, prototyping, or demonstration purposes. The system treats them exactly like any other document, but because they contain no real‑world data they pose no privacy or compliance risk.\n",
            "\n",
            "Below is a **comprehensive guide** that walks you through why you might want synthetic documents, how to design them, how to ingest them safely, and what to watch out for.\n",
            "\n",
            "---\n",
            "\n",
            "## 1. Why create hypothetical documents?\n",
            "\n",
            "| Use‑case | Benefit |\n",
            "|----------|---------|\n",
            "| **Prototype UI/UX** | Quickly populate a search UI so designers can see how results look without waiting for real data. |\n",
            "| **Performance testing** | Generate thousands of documents to benchmark indexing speed, query latency, and storage requirements. |\n",
            "| **Model fine‑tuning** | Provide a controlled corpus for training or fine‑tuning retrieval models (e.g., embeddings) without contaminating the model with sensitive content. |\n",
            "| **Demo / sales** | Show stakeholders a realistic‑looking system without exposing any proprietary or personal information. |\n",
            "| **Compliance sandbox** | Verify that data‑handling pipelines (redaction, encryption, audit logging) work correctly before ingesting production data. |\n",
            "\n",
            "---\n",
            "\n",
            "## 2. Principles for safe, useful synthetic documents\n",
            "\n",
            "| Principle | What it means for your synthetic corpus |\n",
            "|-----------|------------------------------------------|\n",
            "| **Realism** | Use language, structure, and metadata that mimic the target domain (e.g., invoices, legal contracts, medical notes). |\n",
            "| **Determinism (optional)** | If you need reproducible tests, generate documents from a fixed random seed or a deterministic template. |\n",
            "| **No PII/PHI** | Ensure no real personally‑identifiable or protected health information appears. Replace any real names, IDs, or dates with placeholders. |\n",
            "| **Coverage** | Include a variety of document lengths, formats (PDF, DOCX, plain text, HTML), and edge‑cases (missing fields, malformed markup). |\n",
            "| **Labeling** | Tag each synthetic doc with a `synthetic:true` flag in its metadata. This makes it easy to filter them out later if needed. |\n",
            "\n",
            "---\n",
            "\n",
            "## 3. How to generate the documents\n",
            "\n",
            "### 3.1 Simple template‑based approach (quick & low‑tech)\n",
            "\n",
            "```python\n",
            "import random\n",
            "import uuid\n",
            "from datetime import datetime, timedelta\n",
            "\n",
            "TEMPLATES = {\n",
            "    \"invoice\": \"\"\"Invoice #{num}\n",
            "Date: {date}\n",
            "Bill To: {company}\n",
            "Amount Due: ${amount:.2f}\n",
            "Details:\n",
            "{items}\n",
            "Thank you for your business!\"\"\",\n",
            "    \"contract\": \"\"\"SERVICE AGREEMENT\n",
            "Agreement ID: {uid}\n",
            "Effective Date: {date}\n",
            "Parties: {party_a} (\"Provider\") and {party_b} (\"Client\")\n",
            "Scope of Services:\n",
            "{scope}\n",
            "Payment Terms: {payment}\n",
            "Signature: ______________________\n",
            "\"\"\",\n",
            "    # add more as needed\n",
            "}\n",
            "\n",
            "def random_date(start, end):\n",
            "    return start + timedelta(seconds=random.randint(0, int((end-start).total_seconds())))\n",
            "\n",
            "def generate_invoice():\n",
            "    items = \"\\n\".join([f\"- Item {i+1}: ${random.uniform(10, 500):.2f}\" for i in range(random.randint(1,5))])\n",
            "    return TEMPLATES[\"invoice\"].format(\n",
            "        num=random.randint(1000,9999),\n",
            "        date=random_date(datetime(2022,1,1), datetime(2024,12,31)).strftime(\"%Y-%m-%d\"),\n",
            "        company=f\"Acme Corp {random.choice(['LLC','Inc','Ltd'])}\",\n",
            "        amount=sum([float(line.split('$')[1]) for line in items.splitlines()]),\n",
            "        items=items\n",
            "    )\n",
            "\n",
            "def generate_contract():\n",
            "    return TEMPLATES[\"contract\"].format(\n",
            "        uid=str(uuid.uuid4()),\n",
            "        date=random_date(datetime(2022,1,1), datetime(2024,12,31)).strftime(\"%Y-%m-%d\"),\n",
            "        party_a=\"Provider Co.\",\n",
            "        party_b=\"Client Co.\",\n",
            "        scope=\"Provision of consulting services for a period of 12 months.\",\n",
            "        payment=\"Net 30 days upon receipt of invoice.\"\n",
            "    )\n",
            "```\n",
            "\n",
            "*Pros*: Fast, fully controllable, easy to embed domain‑specific jargon.  \n",
            "*Cons*: Limited linguistic diversity; may not stress‑test tokenizers or embeddings as well as real text.\n",
            "\n",
            "---\n",
            "\n",
            "### 3.2 Language‑model‑driven generation (high realism)\n",
            "\n",
            "If you have access to a large language model (LLM) (e.g., OpenAI GPT‑4, Anthropic Claude, or an open‑source model like Llama‑3), you can prompt it to produce realistic documents:\n",
            "\n",
            "```python\n",
            "prompt = \"\"\"Generate a 2‑page legal memorandum about data‑privacy compliance for a mid‑size tech startup. Use standard legal headings, include citations, and embed a few bullet‑point recommendations. Do NOT use any real company names or personal data.\"\"\"\n",
            "response = llm.generate(prompt, temperature=0.7, max_tokens=1500)\n",
            "```\n",
            "\n",
            "*Tips*:\n",
            "\n",
            "- **Set `temperature` low (0.2‑0.4)** for more deterministic output if you need repeatability.\n",
            "- **Add a “synthetic” tag** in the prompt: “At the end of the document, add a line `--- SYNTHETIC DOCUMENT ---`”.\n",
            "- **Post‑process** to strip any accidental real‑world references (run a regex that looks for known brand names, phone numbers, etc.).\n",
            "\n",
            "*Pros*: Highly varied language, better for stress‑testing embeddings.  \n",
            "*Cons*: Requires API credits, and you must still audit the output for accidental leakage of real data.\n",
            "\n",
            "---\n",
            "\n",
            "### 3.3 Data‑augmentation pipelines\n",
            "\n",
            "Combine the two approaches:\n",
            "\n",
            "1. **Start with a base template** (e.g., a contract skeleton).  \n",
            "2. **Replace placeholders** with LLM‑generated paragraphs (e.g., “Scope of Services” section).  \n",
            "3. **Apply random formatting** (bold, tables, bullet lists) to mimic real file diversity.\n",
            "\n",
            "---\n",
            "\n",
            "## 4. Ingesting synthetic docs into Hyde Retriever\n",
            "\n",
            "Hyde’s ingestion pipeline is agnostic to the source of the text, but you’ll want to follow a few best‑practice steps:\n",
            "\n",
            "| Step | Action |\n",
            "|------|--------|\n",
            "| **1. Assign a unique ID** | Use a UUID or a deterministic hash (`sha256(template_name + seed)`). |\n",
            "| **2. Add metadata** | ```json { \"synthetic\": true, \"doc_type\": \"invoice\", \"generation_method\": \"template\", \"generated_at\": \"2025-09-02T12:34:56Z\" } ``` |\n",
            "| **3. Choose a format** | Hyde can ingest plain‑text, PDF, DOCX, HTML, or even raw binary blobs. For realism, mix formats. |\n",
            "| **4. Run the normal pipeline** | `hyde ingest --source ./synthetic_docs/ --metadata ./metadata.json` |\n",
            "| **5. Verify** | After ingestion, run a quick query like `hyde query \"invoice\"` and confirm that synthetic docs appear with the expected scores. |\n",
            "| **6. Flag for later exclusion** | If you ever need to purge synthetic data, a simple `DELETE FROM documents WHERE synthetic = true;` (or the equivalent API call) will do it. |\n",
            "\n",
            "---\n",
            "\n",
            "## 5. Testing & validation checklist\n",
            "\n",
            "| ✅ Item | Why it matters |\n",
            "|--------|----------------|\n",
            "| **Document diversity** – at least 3‑5 types (invoice, email, report, legal memo). | Ensures the retriever’s tokenizer and embedding model see varied token distributions. |\n",
            "| **File‑type mix** – PDF, DOCX, HTML, plain‑text. | Validates the file‑type parsers and OCR fallback paths. |\n",
            "| **Length extremes** – < 100 words, ~10 k words. | Checks index size handling and query latency for long passages. |\n",
            "| **Noise injection** – occasional missing fields, malformed markup. | Tests robustness to real‑world “dirty” data. |\n",
            "| **Metadata correctness** – `synthetic:true` present. | Guarantees you can filter them out later. |\n",
            "| **Search relevance sanity‑check** – known keywords appear in top‑k results. | Confirms that the indexing pipeline correctly tokenizes and stores the content. |\n",
            "\n",
            "---\n",
            "\n",
            "## 6. Common pitfalls & how to avoid them\n",
            "\n",
            "| Pitfall | Symptom | Fix |\n",
            "|---------|---------|-----|\n",
            "| **Accidental real data leakage** | A synthetic doc contains a real email address or phone number. | Run a **PII scanner** (e.g., Presidio, AWS Macie) on the generated corpus before ingestion. |\n",
            "| **Over‑fitting to synthetic style** | Retrieval performance looks great on synthetic queries but drops on real queries. | Always keep a **hold‑out set of real documents** for final evaluation. |\n",
            "| **Missing metadata flag** | You later can’t differentiate synthetic from production docs. | Enforce a **schema validation step** that rejects any doc lacking the `synthetic` flag. |\n",
            "| **Excessive size** | Indexing takes hours or runs out of memory. | Limit the number of generated docs or chunk them into smaller pieces (Hyde supports chunking). |\n",
            "| **Inconsistent encoding** | Some PDFs appear garbled. | Ensure all generated files use UTF‑8 and, for PDFs, embed a standard font (e.g., Helvetica). |\n",
            "\n",
            "---\n",
            "\n",
            "## 7. Example end‑to‑end workflow (Python)\n",
            "\n",
            "```python\n",
            "import os, json, uuid, datetime\n",
            "from hyde import HydeClient   # hypothetical SDK\n",
            "\n",
            "# 1️⃣ Generate a batch of synthetic docs\n",
            "def make_batch(num=200):\n",
            "    docs = []\n",
            "    for i in range(num):\n",
            "        if i % 2 == 0:\n",
            "            txt = generate_invoice()\n",
            "            doc_type = \"invoice\"\n",
            "        else:\n",
            "            txt = generate_contract()\n",
            "            doc_type = \"contract\"\n",
            "\n",
            "        doc_id = str(uuid.uuid4())\n",
            "        filename = f\"{doc_id}.txt\"\n",
            "        with open(os.path.join(\"synthetic_docs\", filename), \"w\", encoding=\"utf-8\") as f:\n",
            "            f.write(txt)\n",
            "\n",
            "        meta = {\n",
            "            \"id\": doc_id,\n",
            "            \"synthetic\": True,\n",
            "            \"doc_type\": doc_type,\n",
            "            \"generation_method\": \"template\",\n",
            "            \"generated_at\": datetime.datetime.utcnow().isoformat() + \"Z\"\n",
            "        }\n",
            "        docs.append(meta)\n",
            "    # write metadata file\n",
            "    with open(\"synthetic_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
            "        json.dump(docs, f, indent=2)\n",
            "\n",
            "# 2️⃣ Ingest\n",
            "def ingest():\n",
            "    client = HydeClient(api_key=\"YOUR_API_KEY\")\n",
            "    client.ingest(\n",
            "        source_path=\"synthetic_docs/\",\n",
            "        metadata_path=\"synthetic_metadata.json\"\n",
            "    )\n",
            "\n",
            "# 3️⃣ Quick sanity query\n",
            "def sanity_check():\n",
            "    client = HydeClient(api_key=\"YOUR_API_KEY\")\n",
            "    results = client.query(\"payment terms\", top_k=5)\n",
            "    for r in results:\n",
            "        print(r[\"id\"], r[\"score\"], r[\"metadata\"][\"synthetic\"])\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    os.makedirs(\"synthetic_docs\", exist_ok=True)\n",
            "    make_batch(100)   # generate 100 docs\n",
            "    ingest()\n",
            "    sanity_check()\n",
            "```\n",
            "\n",
            "*Result*: You’ll see a list of synthetic document IDs with high relevance scores, confirming that the retriever can locate the generated content.\n",
            "\n",
            "---\n",
            "\n",
            "## 8. Legal & ethical considerations\n",
            "\n",
            "| Aspect | Guidance |\n",
            "|--------|----------|\n",
            "| **Copyright** | Since you are generating the text yourself (or via an LLM under a permissible license), you own the output. Avoid copying large verbatim excerpts from copyrighted works. |\n",
            "| **Data‑privacy regulations** (GDPR, CCPA, HIPAA) | Synthetic data is **not** personal data, so it falls outside most privacy obligations. However, if you use an LLM that was trained on proprietary data, verify that the provider’s terms allow commercial generation. |\n",
            "| **Transparency** | If you ever expose synthetic results to end‑users (e.g., in a demo), label them clearly (“Sample data – not real”). |\n",
            "| **Model bias** | Synthetic text can inadvertently reinforce biases if your prompts or templates embed stereotypical language. Review a random sample for fairness. |\n",
            "\n",
            "---\n",
            "\n",
            "## 9. TL;DR – Quick cheat‑sheet\n",
            "\n",
            "1. **Decide the purpose** (testing, demo, performance).  \n",
            "2. **Pick a generation method** – templates for speed, LLM for realism, or a hybrid.  \n",
            "3. **Add a `synthetic:true` flag** in metadata.  \n",
            "4. **Mix formats & lengths** (PDF, DOCX, plain‑text; short & long).  \n",
            "5. **Run a PII scan** before ingestion.  \n",
            "6. **Ingest via the normal Hyde pipeline**.  \n",
            "7. **Validate** with a few known‑keyword queries.  \n",
            "8. **Document** the generation process so you can reproduce or purge the data later.\n",
            "\n",
            "---\n",
            "\n",
            "### Bottom line\n",
            "Creating hypothetical documents is not only possible with Hyde Retriever, it’s a **best practice** for safe development and robust evaluation. By following the steps above you’ll get a realistic‑looking corpus, keep your production data pristine, and stay on the right side of privacy and compliance. Happy testing!\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def interactive_rag():\n",
        "    \"\"\"Interactive function to test the RAG system\"\"\"\n",
        "    while True:\n",
        "        query = input(\"\\nEnter your query (or 'quit' to exit): \")\n",
        "        if query.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        run_rag_query(query)\n",
        "\n",
        "interactive_rag()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "US5Awi25WAO8",
        "outputId": "4f82e6ad-495c-4d51-ea4b-b944d045314c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Enter your query (or 'quit' to exit): \"What is LangChain and how does it work?\"\n",
            "Query: \"What is LangChain and how does it work?\"\n",
            "Retriever Used: similarity\n",
            "Answer: **LangChain – a high‑level framework for building LLM‑powered applications**\n",
            "\n",
            "---\n",
            "\n",
            "## 1. What is LangChain?\n",
            "\n",
            "LangChain is an open‑source Python (and now also JavaScript/TypeScript) framework that makes it easier to **design, develop, and deploy applications that use large language models (LLMs)** such as OpenAI’s GPT‑4, Anthropic’s Claude, LLaMA, Gemini, etc.  \n",
            "\n",
            "Instead of writing ad‑hoc code that stitches together prompt strings, API calls, and post‑processing, LangChain provides **building blocks (components) and patterns** that let you:\n",
            "\n",
            "| Goal | LangChain component that solves it |\n",
            "|------|------------------------------------|\n",
            "| **Prompt engineering** – reusable, templated prompts | `PromptTemplate`, `FewShotPromptTemplate`, `ChatPromptTemplate` |\n",
            "| **LLM invocation** – abstract over different providers | `LLM` wrappers (`OpenAI`, `Anthropic`, `Cohere`, `AzureOpenAI`, …) |\n",
            "| **Chaining** – feed the output of one step into the next | `Chain`, `SequentialChain`, `SimpleSequentialChain` |\n",
            "| **Decision‑making / tool use** – let the model call external functions or APIs | `Agent`, `Tool`, `AgentExecutor` |\n",
            "| **Memory** – preserve context across turns in a conversation | `Memory` classes (`ConversationBufferMemory`, `ConversationSummaryMemory`, `VectorStoreRetrieverMemory`, …) |\n",
            "| **Retrieval‑augmented generation (RAG)** – fetch relevant documents before prompting | `Retriever`, `VectorStore`, `Document`, `RAGChain` |\n",
            "| **Evaluation & testing** – measure correctness, hallucination, latency | `Evaluator`, `CallbackManager` |\n",
            "| **Deployment utilities** – streaming, async, callbacks, tracing | `Callbacks`, `StreamingStdOutCallbackHandler`, `LangServe` |\n",
            "\n",
            "In short, LangChain is **the “glue” that connects LLMs with data sources, logic, and user interfaces**, while providing a clean, modular API that encourages reuse and experimentation.\n",
            "\n",
            "---\n",
            "\n",
            "## 2. Core Concepts & How They Fit Together\n",
            "\n",
            "### 2.1 Prompt Templates\n",
            "- **Purpose:** Define a prompt once and reuse it with different variables.\n",
            "- **How it works:** A template string contains placeholders (`{question}`, `{context}`) that are filled at runtime via `PromptTemplate.format(...)`.  \n",
            "- **Variants:**  \n",
            "  - `ChatPromptTemplate` for multi‑turn chat messages (system, user, assistant).  \n",
            "  - `FewShotPromptTemplate` that automatically adds a few example pairs.\n",
            "\n",
            "### 2.2 LLM Wrappers\n",
            "- **Purpose:** Provide a uniform interface (`.invoke()`, `.stream()`) regardless of the underlying provider.\n",
            "- **How it works:** Each wrapper knows how to call the provider’s API, handle token limits, temperature, streaming, etc.  \n",
            "- **Example:**  \n",
            "  ```python\n",
            "  from langchain.llms import OpenAI\n",
            "  llm = OpenAI(model=\"gpt-4o\", temperature=0.2)\n",
            "  response = llm.invoke(\"Explain LangChain in one sentence.\")\n",
            "  ```\n",
            "\n",
            "### 2.3 Chains\n",
            "- **Purpose:** Sequence multiple steps (prompt → LLM → post‑processing) into a single callable object.\n",
            "- **How it works:** A `Chain` has an `input_keys` list and an `output_keys` list. When you call `chain.run(input)`, LangChain automatically passes the output of each component to the next.\n",
            "- **Typical pattern:**  \n",
            "  1. **Retriever** fetches relevant docs → `retriever.get_relevant_documents(query)`.  \n",
            "  2. **PromptTemplate** inserts docs into a prompt.  \n",
            "  3. **LLM** generates answer.  \n",
            "  4. **Post‑processor** (e.g., `StrOutputParser`) cleans the text.\n",
            "\n",
            "### 2.4 Agents\n",
            "- **Purpose:** Give the LLM the ability to **decide** which tool to call next (search, calculator, database, etc.) and to iterate until a final answer is produced.\n",
            "- **How it works:**  \n",
            "  1. The agent receives a user query.  \n",
            "  2. It constructs a **system prompt** that describes available tools and the format of tool‑calling messages.  \n",
            "  3. The LLM replies with a **tool‑use action** (e.g., `Action: Search[query=\"LangChain\"]`).  \n",
            "  4. LangChain parses the action, executes the corresponding `Tool`, feeds the result back to the LLM, and repeats.  \n",
            "- **Common agents:** `ZeroShotAgent`, `ReactAgent`, `OpenAIFunctionsAgent`.\n",
            "\n",
            "### 2.5 Memory\n",
            "- **Purpose:** Preserve conversational state without re‑sending the entire history each turn.\n",
            "- **How it works:** Memory objects store past inputs/outputs (or summaries, embeddings) and expose a `load_memory_variables()` method that injects the stored context into the next prompt.\n",
            "- **Examples:**  \n",
            "  - `ConversationBufferMemory` – raw transcript.  \n",
            "  - `ConversationSummaryMemory` – a running summary to keep token usage low.  \n",
            "  - `VectorStoreRetrieverMemory` – retrieve relevant past chunks via embeddings.\n",
            "\n",
            "### 2.6 Retrieval‑Augmented Generation (RAG)\n",
            "- **Purpose:** Ground LLM responses in external knowledge (documents, databases, APIs) to reduce hallucination.\n",
            "- **How it works:**  \n",
            "  1. **Document ingestion** → split into chunks → embed with a vector model (e.g., OpenAI embeddings).  \n",
            "  2. Store embeddings in a **VectorStore** (FAISS, Pinecone, Chroma, etc.).  \n",
            "  3. At query time, a **Retriever** performs a similarity search, returning top‑k relevant chunks.  \n",
            "  4. Those chunks are inserted into a prompt (often via a `PromptTemplate`).  \n",
            "  5. LLM generates a response that is *anchored* to the retrieved text.\n",
            "\n",
            "### 2.7 Callbacks & Tracing\n",
            "- **Purpose:** Observe the internal flow (prompt creation, LLM request, tool execution) for debugging, logging, UI streaming, or analytics.\n",
            "- **How it works:** Register a `CallbackHandler` (e.g., `StreamingStdOutCallbackHandler`, `LangChainTracer`) with a `CallbackManager`. The manager notifies the handler at each lifecycle event (`on_chain_start`, `on_llm_end`, `on_tool_error`, …).\n",
            "\n",
            "---\n",
            "\n",
            "## 3. Typical End‑to‑End Workflow\n",
            "\n",
            "Below is a **canonical LangChain pipeline** for a Q&A chatbot that uses RAG and conversation memory.\n",
            "\n",
            "```python\n",
            "# 1️⃣  Imports\n",
            "from langchain.document_loaders import TextLoader\n",
            "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
            "from langchain.embeddings import OpenAIEmbeddings\n",
            "from langchain.vectorstores import FAISS\n",
            "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
            "from langchain.llms import OpenAI\n",
            "from langchain.chains import RetrievalQA\n",
            "from langchain.memory import ConversationBufferMemory\n",
            "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
            "\n",
            "# 2️⃣  Load & index documents\n",
            "loader = TextLoader(\"knowledge_base.txt\")\n",
            "docs = loader.load()\n",
            "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
            "chunks = splitter.split_documents(docs)\n",
            "\n",
            "embeddings = OpenAIEmbeddings()\n",
            "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
            "\n",
            "# 3️⃣  Retriever (top‑k similarity search)\n",
            "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
            "\n",
            "# 4️⃣  Prompt that includes retrieved context + chat history\n",
            "prompt = ChatPromptTemplate.from_messages([\n",
            "    (\"system\", \"You are a helpful assistant. Use the provided context to answer.\"),\n",
            "    MessagesPlaceholder(variable_name=\"history\"),\n",
            "    (\"human\", \"{question}\\n\\nContext:\\n{context}\")\n",
            "])\n",
            "\n",
            "# 5️⃣  LLM (streaming for UI responsiveness)\n",
            "llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.0, streaming=True,\n",
            "            callbacks=[StreamingStdOutCallbackHandler()])\n",
            "\n",
            "# 6️⃣  Memory to keep the conversation\n",
            "memory = ConversationBufferMemory(memory_key=\"history\", return_messages=True)\n",
            "\n",
            "# 7️⃣  Assemble the RetrievalQA chain\n",
            "qa_chain = RetrievalQA.from_chain_type(\n",
            "    llm=llm,\n",
            "    chain_type=\"stuff\",               # simple concat of retrieved docs\n",
            "    retriever=retriever,\n",
            "    return_source_documents=True,\n",
            "    combine_prompt=prompt,\n",
            "    memory=memory\n",
            ")\n",
            "\n",
            "# 8️⃣  Run the chain (in a loop for a chatbot)\n",
            "while True:\n",
            "    user_input = input(\"\\nYou: \")\n",
            "    if user_input.lower() in {\"exit\", \"quit\"}:\n",
            "        break\n",
            "    result = qa_chain({\"question\": user_input})\n",
            "    print(\"\\nAssistant:\", result[\"answer\"])\n",
            "```\n",
            "\n",
            "**What happens under the hood?**\n",
            "\n",
            "1. **Retriever** fetches the 4 most similar chunks to the user’s question.  \n",
            "2. The **prompt template** is rendered with:\n",
            "   - `question` = user input  \n",
            "   - `context` = concatenated retrieved chunks  \n",
            "   - `history` = prior turns from `ConversationBufferMemory`.  \n",
            "3. The **LLM** receives the full prompt, streams tokens back to the console (or UI).  \n",
            "4. The **chain** returns the answer and optionally the source documents for citation.  \n",
            "5. The **memory** updates automatically, so the next turn includes the whole dialogue.\n",
            "\n",
            "---\n",
            "\n",
            "## 4. Why Use LangChain Instead of “Raw” API Calls?\n",
            "\n",
            "| Challenge | Raw API approach | LangChain solution |\n",
            "|-----------|------------------|--------------------|\n",
            "| **Prompt reuse & versioning** | Manual string concatenation → error‑prone | `PromptTemplate` with variables, easy to serialize |\n",
            "| **Switching providers** | Rewrite request code for each vendor | Unified `LLM` interface (`OpenAI`, `Anthropic`, `AzureOpenAI`, …) |\n",
            "| **Multi‑step logic** | Write custom functions that pass data around | `Chain` objects enforce input/output contracts |\n",
            "| **Tool calling / function calling** | Manual parsing of LLM output | `Agent` parses and executes tools automatically |\n",
            "| **Keeping context** | Store whole transcript → token explosion | `Memory` objects (buffer, summary, vector‑store) keep it efficient |\n",
            "| **RAG** | Build embedding store, similarity search, prompt injection yourself | `Retriever` + `VectorStore` + `RAGChain` do it in a few lines |\n",
            "| **Observability** | Add print statements everywhere | Callback system gives structured logs, streaming, tracing |\n",
            "| **Testing & evaluation** | Write bespoke test harnesses | `Evaluator` classes let you benchmark against ground truth datasets |\n",
            "\n",
            "---\n",
            "\n",
            "## 5. Extensibility – Adding Your Own Pieces\n",
            "\n",
            "LangChain is deliberately **component‑oriented**. If you need something custom, you can subclass:\n",
            "\n",
            "```python\n",
            "from langchain.schema import BaseRetriever, Document\n",
            "\n",
            "class MyAPIRetriever(BaseRetriever):\n",
            "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
            "        # call your proprietary search API\n",
            "        results = my_api.search(query, top_k=5)\n",
            "        return [Document(page_content=r[\"text\"], metadata=r) for r in results]\n",
            "```\n",
            "\n",
            "Similarly, you can create:\n",
            "\n",
            "- **Custom Tools** (`class MyTool(BaseTool)`) for database queries, web scraping, etc.  \n",
            "- **Custom Memory** (`class MyMemory(BaseMemory)`) that stores state in Redis or a SQL table.  \n",
            "- **Custom Prompt Templates** that incorporate conditional sections or dynamic examples.\n",
            "\n",
            "All of these plug into existing `Chain` or `Agent` objects without changing the surrounding code.\n",
            "\n",
            "---\n",
            "\n",
            "## 6. Ecosystem & Community\n",
            "\n",
            "- **Official repos:** `langchain` (core), `langchain-community` (community‑contributed integrations), `langchain-experimental` (new features).  \n",
            "- **LangServe:** a lightweight server that turns any LangChain chain/agent into a REST endpoint with OpenAPI docs.  \n",
            "- **LangSmith:** a hosted platform (by the LangChain team) for experiment tracking, evaluation, and production monitoring.  \n",
            "- **Integrations:** Over 30 vector stores, 20 LLM providers, dozens of tools (SQL, Selenium, Wolfram Alpha, etc.).  \n",
            "- **Documentation & tutorials:** “How‑to guides” cover everything from “Chat with PDFs” to “Build a function‑calling agent”.  \n",
            "- **Community:** Discord, GitHub Discussions, weekly webinars, and a growing number of third‑party extensions.\n",
            "\n",
            "---\n",
            "\n",
            "## 7. Summary – How LangChain Works\n",
            "\n",
            "1. **Define data sources** (documents, APIs, databases).  \n",
            "2. **Wrap them** in LangChain components (`VectorStore`, `Retriever`, `Tool`).  \n",
            "3. **Create prompts** with `PromptTemplate` (or chat‑style templates).  \n",
            "4. **Select an LLM** via an `LLM` wrapper.  \n",
            "5. **Compose steps** into a `Chain` or an `Agent` that decides which step to run next.  \n",
            "6. **Add Memory** if you need multi‑turn context.  \n",
            "7. **Run** the chain/agent; LangChain handles:\n",
            "   - Prompt rendering  \n",
            "   - Token‑limit management  \n",
            "   - API calls (including streaming)  \n",
            "   - Tool execution & result injection  \n",
            "   - Callback notifications for logging/monitoring  \n",
            "8. **Deploy** via LangServe, FastAPI, or any serverless platform; optionally monitor with LangSmith.\n",
            "\n",
            "In essence, **LangChain abstracts the plumbing** between large language models and the surrounding ecosystem (data, tools, UI) while giving developers a clean, modular API to experiment, iterate, and ship production‑grade LLM applications quickly.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Enter your query (or 'quit' to exit): what is rag\n",
            "Query: what is rag\n",
            "Retriever Used: similarity\n",
            "Answer: **RAG (Retrieval‑Augmented Generation)** is a pattern for building AI‑driven applications that combine two complementary capabilities:\n",
            "\n",
            "| Component | What it does | Why it matters |\n",
            "|-----------|--------------|----------------|\n",
            "| **Retrieval** | Searches an external knowledge source (a vector store, database, search engine, etc.) for pieces of information that are relevant to the current user query. | Gives the system access to up‑to‑date, factual, or domain‑specific data that the language model alone may not know or may hallucinate about. |\n",
            "| **Generation** | Feeds the retrieved passages (or “documents”) into a large language model (LLM) and asks it to produce a natural‑language answer, summary, or response. | Leverages the LLM’s fluency, reasoning, and ability to stitch together multiple pieces of evidence into a coherent answer. |\n",
            "\n",
            "In short, **RAG = “Retrieval‑Augmented Generation.”** It augments the generative power of an LLM with a retrieval step that grounds the output in real, external information.\n",
            "\n",
            "---\n",
            "\n",
            "## How a RAG pipeline typically works\n",
            "\n",
            "1. **Indexing (offline)**  \n",
            "   * **Ingest** raw data (documents, PDFs, web pages, code, etc.).  \n",
            "   * **Chunk** the data into manageable pieces (e.g., 200‑500 word chunks).  \n",
            "   * **Embed** each chunk with a dense vector model (e.g., OpenAI’s `text‑embedding‑ada‑002`, Sentence‑Transformers, etc.).  \n",
            "   * **Store** the vectors in a similarity index (FAISS, Pinecone, Weaviate, Milvus, etc.) together with the original text.\n",
            "\n",
            "2. **Retrieval & Generation (online, per query)**  \n",
            "   * **Encode** the user’s query into the same embedding space.  \n",
            "   * **Search** the index for the *k* most similar chunks (nearest‑neighbor search).  \n",
            "   * **Pass** those retrieved chunks (often concatenated with the original query) to the LLM.  \n",
            "   * **Generate** a response that is informed by the retrieved evidence.\n",
            "\n",
            "The diagram often looks like:\n",
            "\n",
            "```\n",
            "User Query → Query Encoder → Similarity Search → Retrieved Docs → LLM Prompt → Answer\n",
            "```\n",
            "\n",
            "---\n",
            "\n",
            "## Why use RAG?\n",
            "\n",
            "| Challenge | Traditional LLM‑only approach | RAG solution |\n",
            "|-----------|-------------------------------|--------------|\n",
            "| **Hallucinations / outdated knowledge** | Model may fabricate facts or rely on its static training cut‑off. | Retrieval supplies up‑to‑date, verifiable sources. |\n",
            "| **Domain‑specific expertise** | Fine‑tuning a huge model is costly. | Index a specialized corpus (legal texts, medical guidelines, company docs) and let the LLM use it. |\n",
            "| **Explainability** | Hard to trace why a model said something. | You can show the exact retrieved passages that the answer is based on. |\n",
            "| **Scalability** | Adding new knowledge requires re‑training. | Simply add new documents to the index; no model retraining needed. |\n",
            "| **Cost efficiency** | Running a massive LLM for every query can be expensive. | Retrieval narrows the context, allowing smaller, cheaper LLMs to produce high‑quality answers. |\n",
            "\n",
            "---\n",
            "\n",
            "## Typical Tools & Libraries (as of 2024)\n",
            "\n",
            "| Layer | Popular Options |\n",
            "|-------|-----------------|\n",
            "| **Embedding models** | OpenAI `text-embedding-ada-002`, Cohere, HuggingFace sentence‑transformers (e.g., `all-MiniLM-L6-v2`). |\n",
            "| **Vector stores / indexes** | FAISS (local), Pinecone, Weaviate, Milvus, Qdrant, Chroma. |\n",
            "| **RAG orchestration** | LangChain, LlamaIndex (formerly GPT Index), Haystack, AutoRAG. |\n",
            "| **LLMs for generation** | OpenAI GPT‑4/3.5, Anthropic Claude, Llama‑3, Mistral, Gemini, Cohere Command. |\n",
            "| **Prompt templates** | “Answer the question using only the following sources:” + retrieved docs. |\n",
            "| **Evaluation** | Retrieval metrics (Recall@k, MRR), generation metrics (BLEU, ROUGE), factuality checks (e.g., LLM‑based self‑critique). |\n",
            "\n",
            "---\n",
            "\n",
            "## Minimal Example (Python + LangChain)\n",
            "\n",
            "```python\n",
            "from langchain.embeddings import OpenAIEmbeddings\n",
            "from langchain.vectorstores import FAISS\n",
            "from langchain.llms import OpenAI\n",
            "from langchain.chains import RetrievalQA\n",
            "\n",
            "# 1️⃣ Indexing (run once)\n",
            "documents = [\"...\"]                     # list of raw text strings\n",
            "embeddings = OpenAIEmbeddings()         # uses OpenAI embedding model\n",
            "vectorstore = FAISS.from_texts(documents, embeddings)\n",
            "\n",
            "# 2️⃣ Retrieval‑augmented generation (run per query)\n",
            "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
            "llm = OpenAI(model=\"gpt-4\")\n",
            "qa_chain = RetrievalQA.from_chain_type(\n",
            "    llm=llm,\n",
            "    chain_type=\"stuff\",                 # simple concatenation of docs\n",
            "    retriever=retriever,\n",
            "    return_source_documents=True\n",
            ")\n",
            "\n",
            "query = \"What are the main components of a RAG system?\"\n",
            "answer = qa_chain({\"query\": query})\n",
            "print(answer[\"result\"])                 # generated answer\n",
            "print(answer[\"source_documents\"])       # the docs that grounded it\n",
            "```\n",
            "\n",
            "This snippet mirrors the **two‑stage** flow described in the context: *indexing* (offline) and *retrieval + generation* (online).\n",
            "\n",
            "---\n",
            "\n",
            "## Extensions & Advanced Variants\n",
            "\n",
            "| Variant | What it adds | Typical use‑case |\n",
            "|---------|--------------|-----------------|\n",
            "| **Multi‑step / iterative retrieval** | After an initial answer, the system can issue follow‑up searches (e.g., “search for supporting evidence”). | Complex reasoning, open‑ended research. |\n",
            "| **Conversation‑aware RAG** | The retriever conditions on the dialogue history, enabling context‑aware answers. | Chatbots that remember prior turns. |\n",
            "| **Hybrid search** | Combines dense vector similarity with traditional BM25 keyword search. | Improves recall when terminology varies. |\n",
            "| **Tool‑augmented RAG** | After retrieval, the LLM can call external APIs (e.g., calculators, browsers). | Tasks that need live data or computation. |\n",
            "| **Self‑RAG / Auto‑RAG** | The system automatically decides how many docs to retrieve, whether to re‑rank, etc. | Optimizing latency vs. answer quality. |\n",
            "\n",
            "---\n",
            "\n",
            "## TL;DR Definition\n",
            "\n",
            "> **RAG (Retrieval‑Augmented Generation)** is a design pattern that first **retrieves** relevant pieces of external information (via vector similarity search or other retrieval methods) and then **generates** a natural‑language response using a large language model, thereby grounding the answer in factual data, reducing hallucinations, and enabling up‑to‑date, domain‑specific knowledge without re‑training the model.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Enter your query (or 'quit' to exit): quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MSapio5zWQ74"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}