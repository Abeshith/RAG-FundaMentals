{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7DMSpzDX9RD"
      },
      "outputs": [],
      "source": [
        "!pip install langgraph langchain-core langchain-community langchain_groq transformers sentence-transformers pillow easyocr beautifulsoup4 arxiv pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_community.document_loaders import ArxivLoader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import easyocr"
      ],
      "metadata": {
        "id": "f818P-V6gc3n"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "os.environ['GROQ_API_KEY'] = GROQ_API_KEY"
      ],
      "metadata": {
        "id": "mcLayxvViXAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGroq(model=\"llama3-70b-8192\", temperature=0.1)\n",
        "text_embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "ocr_reader = easyocr.Reader(['en'])"
      ],
      "metadata": {
        "id": "wjc3vhzzikvX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arxiv_queries = [\n",
        "    \"attention is all you need\",  # Transformer paper\n",
        "    \"retrieval augmented generation\", # RAG papers\n",
        "    \"large language models\"  # LLM papers\n",
        "]\n",
        "\n",
        "print(\"Loading ArXiv papers...\")\n",
        "all_arxiv_docs = []\n",
        "\n",
        "for query in arxiv_queries:\n",
        "    loader = ArxivLoader(\n",
        "        query=query,\n",
        "        load_max_docs=2,\n",
        "        doc_content_chars_max=10000\n",
        "    )\n",
        "    docs = loader.load()\n",
        "    all_arxiv_docs.extend(docs)\n",
        "    print(f\"Loaded {len(docs)} papers for query: '{query}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAM7M8soita5",
        "outputId": "ef157754-b910-4264-a9cc-4fc312973cb6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading ArXiv papers...\n",
            "Loaded 3 papers for query: 'attention is all you need'\n",
            "Loaded 3 papers for query: 'retrieval augmented generation'\n",
            "Loaded 3 papers for query: 'large language models'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "knowledge_base = {\n",
        "    \"text_docs\": [doc.page_content[:2000] for doc in all_arxiv_docs],\n",
        "    \"metadata\": [doc.metadata for doc in all_arxiv_docs],\n",
        "    \"image_descriptions\": [\n",
        "        \"Transformer architecture diagram with attention mechanism\",\n",
        "        \"RAG pipeline showing retrieval and generation components\",\n",
        "        \"Large language model neural network structure\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "text_embeddings = text_embedder.encode(knowledge_base[\"text_docs\"])\n",
        "print(f\"Created embeddings for {len(all_arxiv_docs)} ArXiv papers\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RtbywCZi9bx",
        "outputId": "35d2ed10-216b-4d3e-9bd5-11cc2f46d7fb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created embeddings for 9 ArXiv papers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ArxivBiModalState(TypedDict):\n",
        "    messages: Annotated[list[BaseMessage], add_messages]\n",
        "    text_results: Annotated[list[str], lambda x, y: [y]]\n",
        "    image_results: Annotated[list[str], lambda x, y: [y]]\n",
        "    final_response: str\n",
        "    arxiv_sources: list"
      ],
      "metadata": {
        "id": "f8-egrn1jBLY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def arxiv_text_retrieval_agent(state: ArxivBiModalState) -> ArxivBiModalState:\n",
        "    \"\"\"Performs semantic search on ArXiv papers\"\"\"\n",
        "    query = state[\"messages\"][-1].content\n",
        "\n",
        "    query_embedding = text_embedder.encode([query])\n",
        "    similarities = np.dot(query_embedding, text_embeddings.T)[0]\n",
        "    best_match_idx = np.argmax(similarities)\n",
        "\n",
        "    best_doc = knowledge_base[\"text_docs\"][best_match_idx]\n",
        "    metadata = knowledge_base[\"metadata\"][best_match_idx]\n",
        "    similarity_score = similarities[best_match_idx]\n",
        "\n",
        "    title = metadata.get('Title', 'Unknown Title')\n",
        "    authors = metadata.get('Authors', 'Unknown Authors')\n",
        "    published = metadata.get('Published', 'Unknown Date')\n",
        "    arxiv_url = metadata.get('entry_id', 'No URL')\n",
        "\n",
        "    result = f\"\"\"ArXiv Paper Match (Score: {similarity_score:.3f})\n",
        "Title: {title}\n",
        "Authors: {authors}\n",
        "Published: {published}\n",
        "ArXiv ID: {arxiv_url}\n",
        "Content Preview: {best_doc[:400]}...\"\"\"\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"text_results\": [result]  # Return as list\n",
        "    }"
      ],
      "metadata": {
        "id": "C90LvbNcjEhx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def arxiv_image_analysis_agent(state: ArxivBiModalState) -> ArxivBiModalState:\n",
        "    \"\"\"Handles image analysis with ArXiv paper context\"\"\"\n",
        "    query = state[\"messages\"][-1].content\n",
        "\n",
        "    ocr_text = \"Mathematical equations and technical diagrams from research paper\"\n",
        "\n",
        "    query_lower = query.lower()\n",
        "    matching_descriptions = [\n",
        "        desc for desc in knowledge_base[\"image_descriptions\"]\n",
        "        if any(word in desc.lower() for word in query_lower.split())\n",
        "    ]\n",
        "\n",
        "    best_match = matching_descriptions[0] if matching_descriptions else knowledge_base[\"image_descriptions\"][0]\n",
        "\n",
        "    result = f\"ArXiv Image Analysis - OCR: {ocr_text} | Academic Figure: {best_match}\"\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"image_results\": [result]  # Return as list\n",
        "    }"
      ],
      "metadata": {
        "id": "XHNZtHIvjRjT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def arxiv_fusion_agent(state: ArxivBiModalState) -> ArxivBiModalState:\n",
        "    \"\"\"Combines insights from ArXiv papers and images\"\"\"\n",
        "\n",
        "    text_info = state.get(\"text_results\", [\"No text results\"])[0]\n",
        "    image_info = state.get(\"image_results\", [\"No image results\"])[0]\n",
        "\n",
        "    arxiv_sources = [meta.get('entry_id', '') for meta in knowledge_base[\"metadata\"]]\n",
        "\n",
        "    context = f\"\"\"\n",
        "    Query: {state[\"messages\"][-1].content}\n",
        "\n",
        "    ArXiv Paper Analysis: {text_info}\n",
        "    Academic Image Analysis: {image_info}\n",
        "\n",
        "    ArXiv Sources: {arxiv_sources[:3]}\n",
        "    \"\"\"\n",
        "\n",
        "    fusion_prompt = f\"\"\"\n",
        "    Based on the analysis of ArXiv research papers and academic images below, provide a comprehensive response:\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Provide a scholarly answer that leverages information from peer-reviewed research papers.\n",
        "    Include proper academic citations where relevant.\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke([HumanMessage(content=fusion_prompt)])\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"final_response\": response.content,\n",
        "        \"arxiv_sources\": arxiv_sources[:3],\n",
        "        \"messages\": [response]\n",
        "    }"
      ],
      "metadata": {
        "id": "6jbblPhyjVW0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_arxiv_bimodal_graph_sequential():\n",
        "    \"\"\"Build ArXiv-focused bi-modal RAG graph with sequential execution\"\"\"\n",
        "\n",
        "    graph = StateGraph(ArxivBiModalState)\n",
        "\n",
        "    graph.add_node(\"arxiv_text_agent\", arxiv_text_retrieval_agent)\n",
        "    graph.add_node(\"arxiv_image_agent\", arxiv_image_analysis_agent)\n",
        "    graph.add_node(\"arxiv_fusion_agent\", arxiv_fusion_agent)\n",
        "\n",
        "    graph.add_edge(START, \"arxiv_text_agent\")\n",
        "    graph.add_edge(\"arxiv_text_agent\", \"arxiv_image_agent\")\n",
        "    graph.add_edge(\"arxiv_image_agent\", \"arxiv_fusion_agent\")\n",
        "    graph.add_edge(\"arxiv_fusion_agent\", END)\n",
        "\n",
        "    return graph.compile()\n",
        "\n",
        "arxiv_bimodal_agent = build_arxiv_bimodal_graph_sequential()"
      ],
      "metadata": {
        "id": "ZAUAleKmjY6e"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_arxiv_agent(query: str):\n",
        "    \"\"\"Test the ArXiv bi-modal RAG agent\"\"\"\n",
        "\n",
        "    initial_state = {\n",
        "        \"messages\": [HumanMessage(content=query)],\n",
        "        \"text_results\": [],\n",
        "        \"image_results\": [],\n",
        "        \"final_response\": \"\",\n",
        "        \"arxiv_sources\": []\n",
        "    }\n",
        "\n",
        "    result = arxiv_bimodal_agent.invoke(initial_state)\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Query: {query}\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"ArXiv Text Results: {result['text_results'][0][:300]}...\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Academic Image Results: {result['image_results'][0]}\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"ArXiv Sources: {result['arxiv_sources']}\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Scholarly Response: {result['final_response']}\")\n",
        "    print(\"=\" * 70)"
      ],
      "metadata": {
        "id": "plLIU7h7jZpU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Testing ArXiv-Based Bi-Modal RAG Agent\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "test_arxiv_agent(\"Explain the transformer architecture and attention mechanism\")\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "test_arxiv_agent(\"How does retrieval-augmented generation work?\")\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRgwpzVBjidJ",
        "outputId": "90cb72f1-07a7-4591-e845-f1fa307a2111"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing ArXiv-Based Bi-Modal RAG Agent\n",
            "======================================================================\n",
            "======================================================================\n",
            "Query: Explain the transformer architecture and attention mechanism\n",
            "======================================================================\n",
            "ArXiv Text Results: [[['ArXiv Paper Match (Score: 0.417)\\nTitle: RITA: Group Attention is All You Need for Timeseries Analytics\\nAuthors: Jiaming Liang, Lei Cao, Samuel Madden, Zachary Ives, Guoliang Li\\nPublished: 2023-06-02\\nArXiv ID: No URL\\nContent Preview: RITA: Group Attention is All You Need for Timeseries Analytics\\nJiaming Liang\\nUniversity of Pennsylvania\\nPhiladelphia, PA, USA\\nliangjm@seas.upenn.edu\\nLei Caoâˆ—\\nMassachusetts Institute of Technology\\nCambridge, MA, USA\\nlcao@csail.mit.edu\\nSamuel Madden\\nMassachusetts Institute of Technology\\nCambridge, MA, USA\\nmadden@csail.mit.edu\\nZachary Ives\\nUniversity of Pennsylvania\\nPhiladelphia, PA, USA\\nzives@cis.up...']]]...\n",
            "--------------------------------------------------\n",
            "Academic Image Results: [['ArXiv Image Analysis - OCR: Mathematical equations and technical diagrams from research paper | Academic Figure: Transformer architecture diagram with attention mechanism']]\n",
            "--------------------------------------------------\n",
            "ArXiv Sources: ['', '', '']\n",
            "--------------------------------------------------\n",
            "Scholarly Response: The transformer architecture and attention mechanism are fundamental components of deep learning models that have revolutionized the field of natural language processing (NLP) and beyond. In this response, we will delve into the details of these concepts, drawing from the analysis of ArXiv research papers and academic images.\n",
            "\n",
            "**Transformer Architecture**\n",
            "\n",
            "The transformer architecture was introduced by Vaswani et al. (2017) [1] as a novel approach to sequence-to-sequence tasks, such as machine translation. The core idea is to replace traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs) with self-attention mechanisms, enabling parallelization and scalability.\n",
            "\n",
            "The transformer architecture consists of an encoder and a decoder. The encoder takes in a sequence of tokens (e.g., words or characters) and outputs a continuous representation of the input sequence. The decoder generates the output sequence, one token at a time, based on the encoder's output and the previous tokens generated.\n",
            "\n",
            "**Attention Mechanism**\n",
            "\n",
            "The attention mechanism is a crucial component of the transformer architecture. It allows the model to focus on specific parts of the input sequence when generating each output token. This is achieved by computing a weighted sum of the input tokens, where the weights are learned during training.\n",
            "\n",
            "The attention mechanism can be formalized as follows:\n",
            "\n",
            "Let `Q`, `K`, and `V` be the query, key, and value matrices, respectively. The attention weights are computed as:\n",
            "\n",
            "`Attention(Q, K, V) = Concat(head1, ..., headh)W^O`\n",
            "\n",
            "where `headi = Attention(QWQi, KWKi, VWKVi)` and `WQi`, `WKi`, and `WKVi` are learned linear projections.\n",
            "\n",
            "The attention weights are then used to compute a weighted sum of the value matrix `V`, resulting in the output of the attention mechanism.\n",
            "\n",
            "**Group Attention**\n",
            "\n",
            "The paper \"RITA: Group Attention is All You Need for Timeseries Analytics\" by Liang et al. (2023) [2] introduces a novel group attention mechanism, which extends the traditional attention mechanism to handle group-wise relationships in time series data. The authors propose a hierarchical attention framework that captures both local and global patterns in the data.\n",
            "\n",
            "The group attention mechanism is particularly useful in time series analytics, where data often exhibits complex patterns and relationships. By leveraging group attention, the RITA model achieves state-of-the-art performance on various time series forecasting tasks.\n",
            "\n",
            "**Academic Image Analysis**\n",
            "\n",
            "The academic image analysis reveals a transformer architecture diagram with an attention mechanism. The diagram illustrates the self-attention mechanism, where the query, key, and value matrices are computed and used to generate the attention weights. The diagram also highlights the multi-head attention mechanism, which allows the model to jointly attend to information from different representation subspaces.\n",
            "\n",
            "**Conclusion**\n",
            "\n",
            "In conclusion, the transformer architecture and attention mechanism are powerful tools for modeling sequential data. The group attention mechanism, as introduced in the RITA paper, extends the traditional attention mechanism to handle group-wise relationships in time series data. By understanding these concepts, researchers and practitioners can develop more effective deep learning models for a wide range of applications.\n",
            "\n",
            "**References**\n",
            "\n",
            "[1] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in Neural Information Processing Systems, 30.\n",
            "\n",
            "[2] Liang, J., Cao, L., Madden, S., Ives, Z., & Li, G. (2023). RITA: Group Attention is All You Need for Timeseries Analytics. arXiv preprint arXiv:2306. XXXXX.\n",
            "======================================================================\n",
            "\n",
            "================================================================================\n",
            "\n",
            "======================================================================\n",
            "Query: How does retrieval-augmented generation work?\n",
            "======================================================================\n",
            "ArXiv Text Results: [[['ArXiv Paper Match (Score: 0.616)\\nTitle: R^2AG: Incorporating Retrieval Information into Retrieval Augmented Generation\\nAuthors: Fuda Ye, Shuangyin Li, Yongqi Zhang, Lei Chen\\nPublished: 2024-10-30\\nArXiv ID: No URL\\nContent Preview: R2AG: Incorporating Retrieval Information into Retrieval Augmented\\nGeneration\\nFuda Ye1, Shuangyin Li1,*, Yongqi Zhang2, Lei Chen2,3\\n1School of Computer Science, South China Normal University\\n2The Hong Kong University of Science and Technology (Guangzhou)\\n3The Hong Kong University of Science and Technology\\nfudayip@m.scnu.edu.cn, shuangyinli@scnu.edu.cn, yongqizhang@hkust-gz.edu.cn, leichen@cse.ust....']]]...\n",
            "--------------------------------------------------\n",
            "Academic Image Results: [['ArXiv Image Analysis - OCR: Mathematical equations and technical diagrams from research paper | Academic Figure: RAG pipeline showing retrieval and generation components']]\n",
            "--------------------------------------------------\n",
            "ArXiv Sources: ['', '', '']\n",
            "--------------------------------------------------\n",
            "Scholarly Response: Retrieval-augmented generation (RAG) is a novel approach that combines the strengths of retrieval-based and generation-based models to produce more accurate and informative text outputs. According to Ye et al. (2024) [1], RAG involves incorporating retrieval information into the generation process to leverage the benefits of both paradigms.\n",
            "\n",
            "The RAG pipeline, as illustrated in the academic image analysis, consists of two primary components: retrieval and generation. The retrieval component is responsible for fetching relevant information from a large database or knowledge base, while the generation component uses this retrieved information to produce the final output text.\n",
            "\n",
            "The process begins with a input prompt or query, which is used to retrieve a set of relevant documents or passages from the database. These retrieved documents are then processed and condensed into a set of key phrases, entities, or concepts that are relevant to the input prompt. This retrieval information is then fed into the generation component, which uses it to generate the final output text.\n",
            "\n",
            "The generation component can be based on various architectures, such as sequence-to-sequence models or language models. The key innovation of RAG lies in the way it incorporates the retrieval information into the generation process. This can be done through various mechanisms, such as:\n",
            "\n",
            "1. Conditioning the generation model on the retrieved information, allowing it to generate text that is more informed and accurate.\n",
            "2. Using the retrieved information to guide the generation process, such as by providing explicit keywords or entities to include in the output text.\n",
            "3. Integrating the retrieved information into the generation model's input, allowing it to leverage the strengths of both retrieval and generation.\n",
            "\n",
            "By incorporating retrieval information into the generation process, RAG models can produce more accurate and informative text outputs that are better grounded in the available knowledge. This approach has been shown to be effective in various natural language processing tasks, such as text summarization, question answering, and dialogue generation.\n",
            "\n",
            "In conclusion, retrieval-augmented generation is a powerful approach that combines the strengths of retrieval-based and generation-based models to produce more accurate and informative text outputs. By incorporating retrieval information into the generation process, RAG models can leverage the benefits of both paradigms and produce more effective results.\n",
            "\n",
            "Reference:\n",
            "[1] Ye, F., Li, S., Zhang, Y., & Chen, L. (2024). R^2AG: Incorporating Retrieval Information into Retrieval Augmented Generation. arXiv preprint arXiv:XXXX.XXXXX.\n",
            "======================================================================\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uLGSQcHIlnM3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}