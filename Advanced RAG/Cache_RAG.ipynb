{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArZyPIVlhf5f"
      },
      "outputs": [],
      "source": [
        "!pip install langchain chromadb sentence-transformers langchain_groq numpy scikit-learn langchain_chroma langchain_huggingface  langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "os.environ['GROQ_API_KEY'] = GROQ_API_KEY"
      ],
      "metadata": {
        "id": "fUImHJ-fht9u"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize multi-tier cache dictionaries\n",
        "embedding_cache = {}    # Tier 1: Embedding cache\n",
        "search_cache = {}       # Tier 2: Search results cache\n",
        "response_cache = {}     # Tier 3: Generated response cache\n",
        "cache_metadata = {}     # TTL and hit count tracking"
      ],
      "metadata": {
        "id": "DtPKsu1fif2D"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")"
      ],
      "metadata": {
        "id": "jNuRprxrisUr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "llm = ChatGroq(model=\"llama-3.3-70b-versatile\")"
      ],
      "metadata": {
        "id": "CI9jhuL7iyCz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import hashlib\n",
        "\n",
        "def get_cache_key(text):\n",
        "    \"\"\"Generate consistent cache key from text\"\"\"\n",
        "    return hashlib.md5(text.encode()).hexdigest()\n",
        "\n",
        "def is_cache_valid(cache_key, ttl_seconds=3600):\n",
        "    \"\"\"Check if cache entry is still valid based on TTL\"\"\"\n",
        "    if cache_key not in cache_metadata:\n",
        "        return False\n",
        "    timestamp = cache_metadata[cache_key]['timestamp']\n",
        "    return (time.time() - timestamp) < ttl_seconds\n",
        "\n",
        "\n",
        "def update_cache_metadata(cache_key):\n",
        "    \"\"\"Update cache metadata with timestamp and hit count\"\"\"\n",
        "    if cache_key not in cache_metadata:\n",
        "        cache_metadata[cache_key] = {'hits': 0, 'timestamp': time.time()}\n",
        "    cache_metadata[cache_key]['hits'] += 1\n",
        "    cache_metadata[cache_key]['timestamp'] = time.time()"
      ],
      "metadata": {
        "id": "nuvc3I0ojPnK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_with_cache(text):\n",
        "    \"\"\"Embed text with caching layer\"\"\"\n",
        "    cache_key = get_cache_key(text)\n",
        "\n",
        "    if cache_key in embedding_cache and is_cache_valid(cache_key):\n",
        "        update_cache_metadata(cache_key)\n",
        "        return embedding_cache[cache_key]\n",
        "\n",
        "    embedding = embedding_model.embed_query(text)\n",
        "    embedding_cache[cache_key] = embedding\n",
        "    update_cache_metadata(cache_key)\n",
        "    return embedding"
      ],
      "metadata": {
        "id": "Gpp8CUl0jamH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_with_cache(query, retriever):\n",
        "    \"\"\"Retrieve documents with search cache\"\"\"\n",
        "    cache_key = get_cache_key(query)\n",
        "\n",
        "    if cache_key in search_cache and is_cache_valid(cache_key):\n",
        "        update_cache_metadata(cache_key)\n",
        "        return search_cache[cache_key]\n",
        "\n",
        "    # Perform new search\n",
        "    results = retriever.get_relevant_documents(query)\n",
        "    search_cache[cache_key] = results\n",
        "    update_cache_metadata(cache_key)\n",
        "    return results"
      ],
      "metadata": {
        "id": "T9voo9Nfjcqw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "def generate_with_cache(query, retriever):\n",
        "    \"\"\"Generate response with full pipeline caching\"\"\"\n",
        "    cache_key = get_cache_key(query)\n",
        "\n",
        "    if cache_key in response_cache and is_cache_valid(cache_key):\n",
        "        update_cache_metadata(cache_key)\n",
        "        return response_cache[cache_key]\n",
        "\n",
        "    # Generate new response\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever\n",
        "    )\n",
        "    response = qa_chain.invoke(query)\n",
        "    response_cache[cache_key] = response\n",
        "    update_cache_metadata(cache_key)\n",
        "    return response"
      ],
      "metadata": {
        "id": "uZPEv8FBjfM8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "url = \"https://en.wikipedia.org/wiki/Retrieval-augmented_generation\"\n",
        "loader = WebBaseLoader(url)\n",
        "docs = loader.load()\n",
        "\n",
        "texts = []\n",
        "for doc in docs:\n",
        "    content = doc.page_content\n",
        "    chunk_size = 1000\n",
        "    chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]\n",
        "    texts.extend(chunks)\n",
        "\n",
        "print(f\"Loaded {len(texts)} text chunks\")\n",
        "\n",
        "vectorstore = Chroma.from_texts(\n",
        "    texts=texts,\n",
        "    embedding=embedding_model,\n",
        "    collection_name=\"rag_cache_demo\"\n",
        ")\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH9MCnDOjjnz",
        "outputId": "b73a9d2a-4cc3-42cd-e876-0fcf4b072cb9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 27 text chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "queries = [\n",
        "    \"What is Retrieval-Augmented Generation?\",\n",
        "    \"How does RAG work?\",\n",
        "    \"What are the benefits of RAG?\",\n",
        "    \"What is Retrieval-Augmented Generation?\"  # Duplicate to test cache\n",
        "]\n",
        "\n",
        "print(\"=== RAG with Caching Demo ===\\n\")\n",
        "\n",
        "for i, query in enumerate(queries, 1):\n",
        "    print(f\"Query {i}: {query}\")\n",
        "\n",
        "    # Time the retrieval\n",
        "    start_time = time.time()\n",
        "    docs = retrieve_with_cache(query, retriever)\n",
        "    retrieval_time = time.time() - start_time\n",
        "\n",
        "    # Time the generation\n",
        "    start_time = time.time()\n",
        "    response = generate_with_cache(query, retriever)\n",
        "    generation_time = time.time() - start_time\n",
        "\n",
        "    print(f\"Retrieved {len(docs)} documents in {retrieval_time:.3f}s\")\n",
        "    print(f\"Generated response in {generation_time:.3f}s\")\n",
        "    print(f\"Response: {response['result'][:200]}...\\n\") # Access the 'result' key before slicing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZE0V2EpLju5J",
        "outputId": "c9c255ce-0cf0-4e2e-ce4e-a0a2161abb27"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== RAG with Caching Demo ===\n",
            "\n",
            "Query 1: What is Retrieval-Augmented Generation?\n",
            "Retrieved 3 documents in 0.000s\n",
            "Generated response in 0.000s\n",
            "Response: Retrieval-Augmented Generation (RAG) is a technology used in artificial intelligence, particularly in large language models. It is designed to improve the accuracy and reliability of generated text by...\n",
            "\n",
            "Query 2: How does RAG work?\n",
            "Retrieved 3 documents in 0.023s\n",
            "Generated response in 0.961s\n",
            "Response: RAG (not explicitly defined in the text, but presumably \"Retrieval-Augmented Generation\") works by incorporating information retrieval before generating responses. Here's a step-by-step explanation:\n",
            "\n",
            "...\n",
            "\n",
            "Query 3: What are the benefits of RAG?\n",
            "Retrieved 3 documents in 0.020s\n",
            "Generated response in 0.714s\n",
            "Response: The benefits of RAG include:\n",
            "\n",
            "1. Improved accuracy of large language models (LLMs) by incorporating information retrieval before generating responses.\n",
            "2. Reduced need for frequent model retraining, wh...\n",
            "\n",
            "Query 4: What is Retrieval-Augmented Generation?\n",
            "Retrieved 3 documents in 0.000s\n",
            "Generated response in 0.000s\n",
            "Response: Retrieval-Augmented Generation (RAG) is a technology used in artificial intelligence, particularly in large language models. It is designed to improve the accuracy and reliability of generated text by...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clear_all_caches():\n",
        "    \"\"\"Clear all cache tiers\"\"\"\n",
        "    embedding_cache.clear()\n",
        "    search_cache.clear()\n",
        "    response_cache.clear()\n",
        "    cache_metadata.clear()\n",
        "    print(\"All caches cleared\")\n",
        "\n",
        "def get_cache_stats():\n",
        "    \"\"\"Display cache statistics\"\"\"\n",
        "    stats = {\n",
        "        'embedding_cache_size': len(embedding_cache),\n",
        "        'search_cache_size': len(search_cache),\n",
        "        'response_cache_size': len(response_cache),\n",
        "        'total_hits': sum(meta['hits'] for meta in cache_metadata.values())\n",
        "    }\n",
        "    return stats\n",
        "\n",
        "def expire_old_cache(max_age_seconds=3600):\n",
        "    \"\"\"Remove expired cache entries\"\"\"\n",
        "    current_time = time.time()\n",
        "    expired_keys = []\n",
        "\n",
        "    for key, meta in cache_metadata.items():\n",
        "        if (current_time - meta['timestamp']) > max_age_seconds:\n",
        "            expired_keys.append(key)\n",
        "\n",
        "    for key in expired_keys:\n",
        "        embedding_cache.pop(key, None)\n",
        "        search_cache.pop(key, None)\n",
        "        response_cache.pop(key, None)\n",
        "        cache_metadata.pop(key, None)\n",
        "\n",
        "    print(f\"Expired {len(expired_keys)} cache entries\")"
      ],
      "metadata": {
        "id": "hAsA16QClZcy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Cache Statistics ===\")\n",
        "stats = get_cache_stats()\n",
        "for key, value in stats.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "print(\"\\n=== Cache Hit Details ===\")\n",
        "for key, meta in cache_metadata.items():\n",
        "    print(f\"Key: {key[:8]}... | Hits: {meta['hits']} | Age: {time.time() - meta['timestamp']:.1f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPdKMWHakVR5",
        "outputId": "e8a477a5-02c8-45de-cd34-3bb03f5c01be"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Cache Statistics ===\n",
            "embedding_cache_size: 0\n",
            "search_cache_size: 3\n",
            "response_cache_size: 3\n",
            "total_hits: 10\n",
            "\n",
            "=== Cache Hit Details ===\n",
            "Key: c2842ace... | Hits: 6 | Age: 50.6s\n",
            "Key: 28f28084... | Hits: 2 | Age: 51.3s\n",
            "Key: 083ab3f7... | Hits: 2 | Age: 50.6s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test cache performance with repeated queries\n",
        "print(\"=== Cache Performance Test ===\")\n",
        "\n",
        "test_query = \"Explain the main components of RAG\"\n",
        "\n",
        "# First run (no cache)\n",
        "start = time.time()\n",
        "response1 = generate_with_cache(test_query, retriever)\n",
        "first_run_time = time.time() - start\n",
        "\n",
        "# Second run (with cache)\n",
        "start = time.time()\n",
        "response2 = generate_with_cache(test_query, retriever)\n",
        "cached_run_time = time.time() - start\n",
        "\n",
        "print(f\"First run: {first_run_time:.3f}s\")\n",
        "print(f\"Cached run: {cached_run_time:.3f}s\")\n",
        "print(f\"Speed improvement: {first_run_time/cached_run_time:.1f}x faster\")\n",
        "print(f\"Same response: {response1 == response2}\")\n",
        "\n",
        "# Cache management demo\n",
        "print(f\"\\nBefore cleanup: {get_cache_stats()}\")\n",
        "expire_old_cache(max_age_seconds=1)  # Expire very recent for demo\n",
        "print(f\"After cleanup: {get_cache_stats()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxBBDi8Tlefw",
        "outputId": "eb966e73-e767-4945-d72f-3216594718e1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Cache Performance Test ===\n",
            "First run: 0.000s\n",
            "Cached run: 0.000s\n",
            "Speed improvement: 2.4x faster\n",
            "Same response: True\n",
            "\n",
            "Before cleanup: {'embedding_cache_size': 0, 'search_cache_size': 0, 'response_cache_size': 1, 'total_hits': 4}\n",
            "Expired 0 cache entries\n",
            "After cleanup: {'embedding_cache_size': 0, 'search_cache_size': 0, 'response_cache_size': 1, 'total_hits': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Aibj-eCDnfnW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}