{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc65f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import fitz\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c94f70ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"GEMINI_API_KEY\"] = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640fc93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Load the PDF document\n",
    "def load_pdf(path):\n",
    "    loader = PyMuPDFLoader(path)\n",
    "    return loader.load()\n",
    "\n",
    "pdf_path = \"attention-is-all-you-need.pdf\"\n",
    "documents = load_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "319da943",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Split the document into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3a17a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Create embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "919f762d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhes\\AppData\\Local\\Temp\\ipykernel_18684\\2850087926.py:3: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "## 4. Store in Chroma DB\n",
    "vectorstore = Chroma.from_documents(docs, embeddings, persist_directory=\"./pdf_rag_data\")\n",
    "vectorstore.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee7ca7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Load Groq LLM\n",
    "llm = ChatGroq(\n",
    "    model = \"gemma2-9b-it\",\n",
    "    temperature=0.3,\n",
    "    max_tokens=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0bcb66cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Create the RAG QA chain\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a99b30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. Ask Some questions\n",
    "questions = [\n",
    "    \"What is the paper about?\",\n",
    "    \"How Many layers are there in the Transformer model?\",\n",
    "    \"What is the role of attention in the Transformer model?\",\n",
    "    \"What is the number is Encodes and Decodes in the Transformer model?\"\n",
    "    \"What is the main contribution of the paper?\",\n",
    "    \"What are the key components of the Transformer architecture?\",\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dec1402c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What is the paper about?\n",
      "A: This document appears to be an excerpt from a research paper discussing variations of the Transformer architecture. \n",
      "\n",
      "The paper focuses on how different architectural choices impact the performance of a machine translation model (specifically English to German).  It presents a table comparing various Transformer models with different parameters and their performance on a benchmark dataset. \n",
      "\n",
      "\n",
      "Let me know if you have any other questions about this excerpt. \n",
      "\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Q: How Many layers are there in the Transformer model?\n",
      "A: The Transformer model has 6 layers in both the encoder and decoder. \n",
      "\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Q: What is the role of attention in the Transformer model?\n",
      "A: The provided text explains that the Transformer model uses attention to process information. \n",
      "\n",
      "It states that varying the number of attention heads and the attention key and value dimensions affects the model's performance.  \n",
      "\n",
      "It also says that single-head attention is worse than the best setting, implying that multiple attention heads are beneficial. \n",
      "\n",
      "\n",
      "However, it doesn't delve into the specific *role* of attention in the model. \n",
      "\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Q: What is the number is Encodes and Decodes in the Transformer model?What is the main contribution of the paper?\n",
      "A: According to the text, the Transformer model has **N = 6** identical layers in both the encoder and decoder. \n",
      "\n",
      "\n",
      "The text does not state the main contribution of the paper. \n",
      "\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Q: What are the key components of the Transformer architecture?\n",
      "A: This text focuses on evaluating the importance of attention heads and key/value dimensions within the Transformer architecture.  \n",
      "\n",
      "It doesn't list all the key components of the Transformer. \n",
      "\n",
      "\n",
      "---------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for q in questions:\n",
    "    result = rag_chain({\"query\": q})\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"A: {result['result']}\")\n",
    "    print(\"---------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900e32fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
