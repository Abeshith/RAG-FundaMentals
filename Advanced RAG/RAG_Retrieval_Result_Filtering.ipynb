{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Mce9IhNGnKm"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-groq langchain-community langchain-chroma\n",
        "!pip install sentence-transformers chromadb beautifulsoup4 requests\n",
        "!pip install groq python-dateutil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import datetime\n",
        "import hashlib\n",
        "import re"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlgMhCslGu3C",
        "outputId": "71a1d00b-6957-422a-839d-1d2f69ad272d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['GROQ_API_KEY'] = userdata.get('GROQ_API_KEY')"
      ],
      "metadata": {
        "id": "fojH28vfJGs4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/Artificial_intelligence\")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "hL66bDfmHFoZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "4Hw0AVnpIOSu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = Chroma.from_documents(chunks, embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixSTmPUsIP2l",
        "outputId": "a362ae9e-fe71-4820-bbf2-f80c5ec11197"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2864554351.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the main applications of artificial intelligence?\"\n",
        "initial_results = vectorstore.similarity_search_with_score(query, k=20)\n",
        "\n",
        "print(f\"Initial results: {len(initial_results)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKx_bXCMIRVZ",
        "outputId": "2f528087-be42-4e3b-f16f-ec92b274e35a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial results: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_duplicates(results):\n",
        "    seen_hashes = set()\n",
        "    filtered = []\n",
        "\n",
        "    for doc, score in results:\n",
        "        content_hash = hashlib.md5(doc.page_content.encode()).hexdigest()\n",
        "        if content_hash not in seen_hashes:\n",
        "            seen_hashes.add(content_hash)\n",
        "            filtered.append((doc, score))\n",
        "\n",
        "    return filtered\n",
        "\n",
        "deduplicated = remove_duplicates(initial_results)\n",
        "print(f\"After deduplication: {len(deduplicated)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX8kgCs8IvhC",
        "outputId": "1d3599e7-f81c-4bce-f297-292ea1ffbe5c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After deduplication: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_by_relevance(results, threshold=0.5):\n",
        "    return [(doc, score) for doc, score in results if score <= threshold]\n",
        "\n",
        "relevance_filtered = filter_by_relevance(deduplicated, threshold=0.8)\n",
        "print(f\"After relevance filtering: {len(relevance_filtered)}\")\n",
        "\n",
        "# 3. CONTENT QUALITY SCORING\n",
        "def quality_score(text):\n",
        "    \"\"\"Simple content quality scoring based on heuristics\"\"\"\n",
        "    score = 0\n",
        "\n",
        "    # Length scoring (moderate length preferred)\n",
        "    length = len(text.split())\n",
        "    if 50 <= length <= 500:\n",
        "        score += 2\n",
        "    elif length > 20:\n",
        "        score += 1\n",
        "\n",
        "    # Sentence structure scoring\n",
        "    sentences = text.split('.')\n",
        "    if len(sentences) >= 2:\n",
        "        score += 1\n",
        "\n",
        "    # Information density (avoid repetitive content)\n",
        "    unique_words = len(set(text.lower().split()))\n",
        "    total_words = len(text.split())\n",
        "    if total_words > 0 and unique_words / total_words > 0.5:\n",
        "        score += 1\n",
        "\n",
        "    return score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hS9EF6oIIywv",
        "outputId": "cbcb726f-a958-4197-e211-6dffc305af0f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After relevance filtering: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quality_filtered = []\n",
        "for doc, score in relevance_filtered:\n",
        "    quality = quality_score(doc.page_content)\n",
        "    if quality >= 2:  # Minimum quality threshold\n",
        "        quality_filtered.append((doc, score, quality))\n",
        "\n",
        "print(f\"After quality filtering: {len(quality_filtered)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gyno3zFDI2vQ",
        "outputId": "947ead68-cb4e-49ac-8162-05aa5e81049d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After quality filtering: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\", temperature=0)\n",
        "\n",
        "def llm_quality_filter(doc_score_pairs, max_docs=10):\n",
        "    \"\"\"Use LLM to assess content quality and relevance\"\"\"\n",
        "    filtered_results = []\n",
        "\n",
        "    for doc, score in doc_score_pairs[:max_docs]:  # Limit LLM calls\n",
        "        # Create quality assessment prompt\n",
        "        prompt = f\"\"\"\n",
        "        Assess this text for quality and relevance to the query: \"{query}\"\n",
        "\n",
        "        Text: {doc.page_content[:500]}\n",
        "\n",
        "        Rate from 1-5 where:\n",
        "        5 = Highly relevant and informative\n",
        "        4 = Relevant with good information\n",
        "        3 = Somewhat relevant\n",
        "        2 = Low relevance\n",
        "        1 = Not relevant\n",
        "\n",
        "        Respond with only the number (1-5):\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = llm.invoke(prompt)\n",
        "            quality_score = int(response.content.strip())\n",
        "\n",
        "            if quality_score >= 3:\n",
        "                filtered_results.append((doc, score, quality_score))\n",
        "        except:\n",
        "            filtered_results.append((doc, score, 3))\n",
        "\n",
        "    return filtered_results\n",
        "\n",
        "llm_filtered = llm_quality_filter(relevance_filtered[:10])\n",
        "print(f\"After LLM quality filtering: {len(llm_filtered)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUQDv0TtI489",
        "outputId": "9e663418-8960-4116-a5bc-dd318dc97117"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After LLM quality filtering: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def complete_filtering_pipeline(query, vectorstore, top_k=20):\n",
        "    \"\"\"Complete filtering pipeline in one function\"\"\"\n",
        "\n",
        "    # Step 1: Initial retrieval\n",
        "    results = vectorstore.similarity_search_with_score(query, k=top_k)\n",
        "\n",
        "    # Step 2: Remove duplicates\n",
        "    seen_hashes = set()\n",
        "    deduped = []\n",
        "    for doc, score in results:\n",
        "        content_hash = hashlib.md5(doc.page_content.encode()).hexdigest()\n",
        "        if content_hash not in seen_hashes:\n",
        "            seen_hashes.add(content_hash)\n",
        "            deduped.append((doc, score))\n",
        "\n",
        "    # Step 3: Relevance threshold\n",
        "    relevance_filtered = [(doc, score) for doc, score in deduped if score <= 0.8]\n",
        "\n",
        "    # Step 4: Content quality heuristics\n",
        "    quality_filtered = []\n",
        "    for doc, score in relevance_filtered:\n",
        "        length = len(doc.page_content.split())\n",
        "        unique_ratio = len(set(doc.page_content.lower().split())) / max(len(doc.page_content.split()), 1)\n",
        "\n",
        "        if 20 <= length <= 1000 and unique_ratio > 0.4:\n",
        "            quality_filtered.append((doc, score))\n",
        "\n",
        "    # Step 5: Sort by relevance score (lower is better for similarity)\n",
        "    final_results = sorted(quality_filtered, key=lambda x: x[1])[:5]\n",
        "\n",
        "    return final_results\n",
        "\n",
        "# Run complete pipeline\n",
        "filtered_results = complete_filtering_pipeline(query, vectorstore)\n",
        "\n",
        "print(\"=== FINAL FILTERED RESULTS ===\")\n",
        "for i, (doc, score) in enumerate(filtered_results):\n",
        "    print(f\"\\n--- Result {i+1} (Relevance: {score:.3f}) ---\")\n",
        "    print(doc.page_content[:200] + \"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvnKtY4FJMhk",
        "outputId": "dbee3544-5fd9-438c-ed29-2f2aa455a57d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== FINAL FILTERED RESULTS ===\n",
            "\n",
            "--- Result 1 (Relevance: 0.688) ---\n",
            "High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Si...\n",
            "\n",
            "--- Result 2 (Relevance: 0.704) ---\n",
            "Applications\n",
            "Main article: Applications of artificial intelligenceAI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Goog...\n",
            "\n",
            "--- Result 3 (Relevance: 0.732) ---\n",
            "General intelligence\n",
            "A machine with artificial general intelligence would be able to solve a wide variety of problems with breadth and versatility similar to human intelligence.[68]\n",
            "\n",
            "Techniques\n",
            "AI res...\n",
            "\n",
            "--- Result 4 (Relevance: 0.751) ---\n",
            "Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, plann...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_filtering_impact(original_results, filtered_results):\n",
        "    print(f\"ðŸ“Š FILTERING ANALYSIS\")\n",
        "    print(f\"Original results: {len(original_results)}\")\n",
        "    print(f\"Final results: {len(filtered_results)}\")\n",
        "    print(f\"Reduction: {((len(original_results) - len(filtered_results)) / len(original_results) * 100):.1f}%\")\n",
        "\n",
        "    # Score distribution analysis\n",
        "    original_scores = [score for _, score in original_results]\n",
        "    filtered_scores = [score for _, score in filtered_results]\n",
        "\n",
        "    print(f\"Original avg score: {sum(original_scores)/len(original_scores):.3f}\")\n",
        "    print(f\"Filtered avg score: {sum(filtered_scores)/len(filtered_scores):.3f}\")\n",
        "\n",
        "analyze_filtering_impact(initial_results, filtered_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWXOzbQdJRnK",
        "outputId": "0a32ca17-7ba6-4ecb-e0b7-c05d9ec10452"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“Š FILTERING ANALYSIS\n",
            "Original results: 20\n",
            "Final results: 4\n",
            "Reduction: 80.0%\n",
            "Original avg score: 0.836\n",
            "Filtered avg score: 0.719\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eypnKHKIJVjI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}